{
  "hash": "d3a5049cc5da16bee909d050d020390f",
  "result": {
    "markdown": "---\ntitle: \"명령라인 데이터 분석\"\ndescription: \"Command Line 방식으로 데이터를 분석하는 방법을 일별한다.\"\ndate: today # 날짜 설정: now, last-modified\nauthor:\n  - name: 이광춘\n    affiliation: TCS\ntitle-block-banner: false\nformat:\n  html:\n    theme: \n      - css/quarto-fonts.css\n    code-fold: false\n    toc: true\n    toc-depth: 2\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nbibliography: bibliography.bib\nlink-citations: yes\ncsl: apa-single-spaced.csl\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# 웹소설 텍스트\n\n저작권 걱정없이 저작물을 받을 수 있는 경로는 여러 곳이 존재한다.\n하지만, PDF, HWP, TXT 파일은 압축하여 제공하고 있어 사람손이 몇번씩 가는 문제점이 있다.\n작업과정에 추가로 프로세스를 넣어주어야만 된다. 어차치 TXT로 작업하는데 ...\n\n대표적 국내외 공유 저작물 저장소 [^public-domain] [^public-domain-summary]로 \n다음을 꼽을 수 있다.\n \n* 미국: [구텐베르크(Gutenberg) 프로젝트](http://www.gutenberg.org/)\n* 일본: [일본판 구텐베르크, 아오조라 문고(靑空文庫, あおぞらぶんこ)](http://www.aozora.gr.jp/)\n* **대한민국**\n   * [공공누리 포털](http://www.kogl.or.kr/)\n   * [직지(http://www.jikji.org/)](http://www.jikji.org/)\n   * [공유마당](http://gongu.copyright.or.kr/ )\n\n[^public-domain]: [공유마당](http://gongu.copyright.or.kr/)\n[^public-domain-summary]: [저작권 걱정없이 이용하기 프로젝트 03. 해외만료저작물 ](http://minheeblog.tistory.com/49)\n\n# 텍스트 분석\n\n[직지(http://www.jikji.org/)](http://www.jikji.org/)에서 수작업으로 `.txt` 파일을 생성하여 `.txt` 파일을 웹에 올려 `curl` 명령어를 통해 바로 다운로드 받게 소설데이터를 준비했다. 소설 데이터는 **B사감과 러브레터** 고등학교 인문계에서 필독서로 아주 오래전에 읽었던 기억이 난다. 영화로도 만들어지고, TV에서도 방영되었던 것으로 기억된다.\n\n\n1. `curl` 명령어를 통해 [https://raw.githubusercontent.com/statkclee/ml/gh-pages/data/B사감과_러브레터.txt](https://raw.githubusercontent.com/statkclee/ml/gh-pages/data/B%EC%82%AC%EA%B0%90%EA%B3%BC_%EB%9F%AC%EB%B8%8C%EB%A0%88%ED%84%B0.txt) 파일을 다운로드 한다.\n1. `grep` 명령어로 정규표현식 패턴을 넣어 단어를 각 행별로 추출하여 쭉 정리해 나간다.\n1. `sort` 명령어로 오름차순으로 정리한다.\n1. `unique` 명령어로 중복을 제거하고 `-c` 인자플래그를 넣어 중복수를 센다.\n1. `sort` 명령어로 단어갯수를 내림차순으로 정리한다.\n1. `head` 명령어로 가장 빈도가 높은 단어 5개를 추출한다. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n$ curl -s https://raw.githubusercontent.com/statkclee/ml/gh-pages/data/B사감과_러브레터.txt | \\\ngrep -oE '\\w+' | \\\nsort | \\\nuniq -c | \\\nsort -nr | \\\nhead -n 5\n```\n:::\n\n\n```\n    138 처음\n    132 직지에\n     65 러브레터\n     47 때\n     26 여학교에서\n```\n\n만약 두도시 이야기(A Tale of Two Cities)에서 가장 많은 단어를 분석하고자 하는 경우 [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)을 인자로 바꿔 넣으면 된다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n$ curl -s http://www.gutenberg.org/cache/epub/98/pg98.txt | \\\ngrep -oE '\\w+' | \\\nsort | \\\nuniq -c | \\\nsort -nr | \\\nhead -n 5\n```\n:::\n\n\n```\n   7577 the\n   4921 and\n   4103 of\n   3601 to\n   2864 a\n```\n\n# 명령라인 데이터 분석 [^cmd-data-analysis] [^data-science-toolbox]\n\n[^cmd-data-analysis]: [Data Science at the Command Line](http://datascienceatthecommandline.com/)\n[^data-science-toolbox]: [Data Science Toolbox](http://datasciencetoolbox.org/)\n\n명령라인 인터페이스를 사용하면, 애자일(Agile), 다른 기술과 증강(Augmenting)이 가능하며, 확장성(Scalable)이 크며, 연장가능(Extensible)하며, 어디서나 사용(Ubiquitous)되는 장점을 갖는다.\n\n유닉스는 **텍스트(Text)** 가 어디서나 사용되는 인터페이스로, 각 개별 구성요소는 한가지 작업만 매우 잘 처리하게 설계되었고, 복잡하고 난이도가 있는 작업은 한가지 작업만 잘 처리하는 것을 **파이프와 필터** 로 자동화하고, 그리고 **쉘스크립트** 를 통해 추상화한다.\n\n## 데이터 가져오기\n\n데이터를 가져오는 방식은 결국 텍스트로 유닉스/리눅스 환경으로 불러와야만 된다.\n**[csvkit](http://csvkit.readthedocs.io/)** 에 `in2csv`, `csvcut`, `csvlook`, `sql2csv`, `csvsql`이\n포함되어 있다. \n\n`sudo pip install csvkit` 명령어로 설치한다.\n\n* 로컬 파일: `cp` 복사, 원격파일 복사: `scp` 복사\n* 압축파일: `tar`, `unzip`, `unrar` 명령어로 압축된 파일을 푼다.\n    * 압축파일 확장자: `.tar.gz`, `.zip`, `.rar`\n    * 압축파일 푸는 종결자 `unpack`\n* 스프레드쉬트: [in2csv](http://csvkit.readthedocs.io/)는 표형식 엑셀 데이터를 받아 `csv` 파일로 변환.\n    * `$ in2csv ne_1033_data.xlsx | csvcut -c county,item_name,quantity | csvlook | head`\n* 데이터베이스: sql2csv\n    * `sql2csv --db 'sqlite:///iris.db' --query 'SELECT * FROM iris where petal_length > 6.5' | csvlook`\n* 인터넷: [curl](https://curl.haxx.se/)을 활용하여 인터넷 자원을 긁어온다.\n    * `curl -s http://www.gutenberg.org/files/13693/13693-t/13693-t.tex -o number-theory.txt`    \n* API: [curl](https://curl.haxx.se/) 물론, API 토큰, 비밀키 등을 설정하거나 일일 이용한도가 있을 수도 있다. 특히, [curlicue](https://github.com/decklin/curlicue)를 활용하여 트위터 데이터를 바로 가져와서 활용할 수 있다. 자세한 사항은 [Create Your Own Dataset Consuming Twitter API](http://arjon.es/2015/07/30/create-your-own-dataset-consuming-twitter-api/) 블로그를 참조한다.\n    * [RANDOM USER GENERATOR](https://randomuser.me/), `curl -s http://api.randomuser.me | jq '.'`\n\n## 데이터 정제\n\n### 행 뽑아내기\n\n* 행 위치정보를 기반으로 해서 행 절대번호를 활용하여 추출한다.\n    * `head`, `sed`, `awk`\n* 패턴을 주고 연관된 행만 추출한다.\n    * `grep` 명령어에 정규표현식으로 패턴을 담아 매칭되는 것만 뽑아낸다.\n    * 사용례: `grep -i session paper.txt`\n* 무작위로 행을 추출한다.\n    * `shuf` 명령어를 사용한다. \n    * 사용례: `shuf -n 10 data.csv` \n\n### 값 추출\n\n기본적인 값추출 전략은 `grep` 명령어로 행을 뽑아내고, `cut` 명령어로 구분자를 두거나 고정된 열위치에 해당하는 열에서 값을 추출한다.\n`cut` 명령어로 열을 쪼개는데 구분자로 `,`를 사용하고 뽑아내는 열로 `-f` 인자를 두고 3번째 행이후 모두를 지정한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n$ grep -i session paper.txt | cut -d ',' -f3-\n$ grep -i session paper.txt | cut -c 7-\n```\n:::\n\n\n### 값 바꾸기\n\n값을 바꾸거나 삭제할 때 사용하는 명령어가 `tr`로 `translate` 번역의 약자다.\n\n공백 ` `을 `*`로 바꾼다.\n\n```\n$ echo 'We Love Data Science!' | tr ' ' '*'\nWe*Love*Data*Science!\n```\n\n\n\n# 명령라인 터미널 동영상 제작\n\n[asciinema (as-kee-nuh-muh)](https://asciinema.org/) 활용하여 쉘 데이터 분석을 동영상 제작할 수 있다.\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\n$ asciinema -yt \"Start Here !!!\"\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}