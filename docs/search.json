[
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "환경설정",
    "section": "",
    "text": "Visual Studio Code: https://code.visualstudio.com/download\nRStudio: https://posit.co/download/rstudio-desktop/\nJupyter: https://jupyter.org/install\n\n\n\n\n\nGit for Windows: https://gitforwindows.org/\n\n\n\n\n\nR: https://cran.r-project.org/bin/windows/base/\n파이썬: https://www.python.org/downloads/\n\n\n\n\n\nQuarto: https://quarto.org/docs/download/\n\n\n\n\n\nSqlite3: https://www.sqlite.org/index.html\nDuckDB: https://duckdb.org/\nPostgreSQL: https://www.postgresql.org/download/\n\n\n\n\n\n텐서플로우: https://www.tensorflow.org/install\n파이토치: https://pytorch.org/\nHuggingface: https://huggingface.co/"
  },
  {
    "objectID": "00_setup.html#전자책",
    "href": "00_setup.html#전자책",
    "title": "환경설정",
    "section": "3.1 전자책",
    "text": "3.1 전자책\n\nBitBook: https://r2bit.com/book"
  },
  {
    "objectID": "00_setup.html#블로그",
    "href": "00_setup.html#블로그",
    "title": "환경설정",
    "section": "3.2 블로그",
    "text": "3.2 블로그\n\n데이터 과학: https://statkclee.github.io/data-science/\n시각화: https://statkclee.github.io/viz/\n문서제작: http://aispiration.com/comp_document/\n\n쿼토(Quarto): https://r2bit.com/quarto/\n파워포인트(PPT): https://r2bit.com/bitSlide/\n웹슬라이드(Xaringan): http://aispiration.com/ds-authoring/\n\n데이터 과학 제품: http://aispiration.com/data-product/\n딥러닝 기본기 : https://aispiration.com/united-states/\n…"
  },
  {
    "objectID": "01_setup_scenario.html",
    "href": "01_setup_scenario.html",
    "title": "CLI 개발환경",
    "section": "",
    "text": "윈도우 환경에서 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구.\n\nGit for Windows: https://gitforwindows.org/\n\n\n\n\n윈도우 10/11에서 WSL(Windows Subsystem Linux)을 설치하여 윈도우에서 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구\n\nWindows Subsystem for Linux (WSL) : https://learn.microsoft.com/en-us/windows/wsl/install\n\n\n\n\n격리 가상화 기술을 사용하여 운영체제에 독립적으로 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구\n\n도커: https://www.docker.com/\n\n명령라인 데이터 분석 작업을 수행하기 위한 환경을 개별적으로 구축하기 보다 이미 구축된 도커 환경을 가져와서 준비한다. [@janssens2021data]\n\n도커를 다운로드 받아 설치한다.\n도커허브에서 도커이미지를 받아온다.\n\ndatasciencetoolbox/dsatcl2e\n\n도커를 실행시키고 로컬 디렉토리와 도커 컨테이너 디렉토리를 일치시킨다.\n\n\n\n\n도커 로컬 디렉토리와 컨테이너 디렉토리 마운트 작업\n\n\n$ docker pull datasciencetoolbox/dsatcl2e \n$ docker run --rm -it datasciencetoolbox/dsatcl2e\n$ docker run --rm -it -v \"$(pwd)\":/home/dst/data datasciencetoolbox/dsatcl2e\n\n\n\n데이터 과학에 특화된 클라우드 서비스를 사용하여 운영체제에 독립적으로 유닉스/리눅스 쉘 명령어는 물론 데이터 과학에 필요한 다양한 기능을 지원하는 서비스\n\nPosit Cloud (formerly RStudio Cloud): https://posit.cloud/\n빛에듀: http://bit-edu.iptime.org/rstudio/"
  },
  {
    "objectID": "10_cmdline.html",
    "href": "10_cmdline.html",
    "title": "명령라인 데이터 분석",
    "section": "",
    "text": "저작권 걱정없이 저작물을 받을 수 있는 경로는 여러 곳이 존재한다. 하지만, PDF, HWP, TXT 파일은 압축하여 제공하고 있어 사람손이 몇번씩 가는 문제점이 있다. 작업과정에 추가로 프로세스를 넣어주어야만 된다. 어차치 TXT로 작업하는데 …\n대표적 국내외 공유 저작물 저장소 1 2로 다음을 꼽을 수 있다.\n\n미국: 구텐베르크(Gutenberg) 프로젝트\n일본: 일본판 구텐베르크, 아오조라 문고(靑空文庫, あおぞらぶんこ)\n대한민국\n\n공공누리 포털\n직지(http://www.jikji.org/)\n공유마당"
  },
  {
    "objectID": "10_cmdline.html#데이터-가져오기",
    "href": "10_cmdline.html#데이터-가져오기",
    "title": "명령라인 데이터 분석",
    "section": "3.1 데이터 가져오기",
    "text": "3.1 데이터 가져오기\n데이터를 가져오는 방식은 결국 텍스트로 유닉스/리눅스 환경으로 불러와야만 된다. csvkit 에 in2csv, csvcut, csvlook, sql2csv, csvsql이 포함되어 있다.\nsudo pip install csvkit 명령어로 설치한다.\n\n로컬 파일: cp 복사, 원격파일 복사: scp 복사\n압축파일: tar, unzip, unrar 명령어로 압축된 파일을 푼다.\n\n압축파일 확장자: .tar.gz, .zip, .rar\n압축파일 푸는 종결자 unpack\n\n스프레드쉬트: in2csv는 표형식 엑셀 데이터를 받아 csv 파일로 변환.\n\n$ in2csv ne_1033_data.xlsx | csvcut -c county,item_name,quantity | csvlook | head\n\n데이터베이스: sql2csv\n\nsql2csv --db 'sqlite:///iris.db' --query 'SELECT * FROM iris where petal_length > 6.5' | csvlook\n\n인터넷: curl을 활용하여 인터넷 자원을 긁어온다.\n\ncurl -s http://www.gutenberg.org/files/13693/13693-t/13693-t.tex -o number-theory.txt\n\n\nAPI: curl 물론, API 토큰, 비밀키 등을 설정하거나 일일 이용한도가 있을 수도 있다. 특히, curlicue를 활용하여 트위터 데이터를 바로 가져와서 활용할 수 있다. 자세한 사항은 Create Your Own Dataset Consuming Twitter API 블로그를 참조한다.\n\nRANDOM USER GENERATOR, curl -s http://api.randomuser.me | jq '.'"
  },
  {
    "objectID": "10_cmdline.html#데이터-정제",
    "href": "10_cmdline.html#데이터-정제",
    "title": "명령라인 데이터 분석",
    "section": "3.2 데이터 정제",
    "text": "3.2 데이터 정제\n\n3.2.1 행 뽑아내기\n\n행 위치정보를 기반으로 해서 행 절대번호를 활용하여 추출한다.\n\nhead, sed, awk\n\n패턴을 주고 연관된 행만 추출한다.\n\ngrep 명령어에 정규표현식으로 패턴을 담아 매칭되는 것만 뽑아낸다.\n사용례: grep -i session paper.txt\n\n무작위로 행을 추출한다.\n\nshuf 명령어를 사용한다.\n사용례: shuf -n 10 data.csv\n\n\n\n\n3.2.2 값 추출\n기본적인 값추출 전략은 grep 명령어로 행을 뽑아내고, cut 명령어로 구분자를 두거나 고정된 열위치에 해당하는 열에서 값을 추출한다. cut 명령어로 열을 쪼개는데 구분자로 ,를 사용하고 뽑아내는 열로 -f 인자를 두고 3번째 행이후 모두를 지정한다.\n\n$ grep -i session paper.txt | cut -d ',' -f3-\n$ grep -i session paper.txt | cut -c 7-\n\n\n\n3.2.3 값 바꾸기\n값을 바꾸거나 삭제할 때 사용하는 명령어가 tr로 translate 번역의 약자다.\n공백 을 *로 바꾼다.\n$ echo 'We Love Data Science!' | tr ' ' '*'\nWe*Love*Data*Science!"
  },
  {
    "objectID": "11_shell.html",
    "href": "11_shell.html",
    "title": "유닉스 쉘(Shell)",
    "section": "",
    "text": "1 유닉스 쉘"
  },
  {
    "objectID": "12_writing.html",
    "href": "12_writing.html",
    "title": "디지털 글쓰기(보고서)",
    "section": "",
    "text": "데이터 과학을 구성하고 있는 데이터, 코드, 시각화 그래프, 보고서, 논문, 모형 등 가치사슬(Value Chain)을 실무적으로 살펴보면 다음과 같다. 즉, 최종 데이터 과학 결과물만 가치가 있고 그 준비과정과 중간에 산출되는 결과물은 비용으로 간주된다. 이미지 출처: David Robinson (@drrob) 발표내용을 Amelia McNamara 박사가 사진 촬영한 것\n\n\n\n데이터 과학 콘텐츠 가치 사슬\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    A[.ipynb] --> B((\"Pandoc\"))\n    B ----> E[.doc]\n    B ----> H[.pptx]\n    B --> C[.md]\n    B --> D[.tex] \n    D --> F((Xetex))\n    C --> I((Hugo))\n    F --> G[.pdf]\n    I --> J[.html]\n    style B fill:#FF6655AA\n    style F fill:#88ffFF\n    style I fill:#88ffFF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\nQ[.qmd] --> A\nsubgraph \"쿼토(Quarto)\"\n    A[.ipynb] --> B((\"Pandoc\"))\n    B --> C[.md]\n    B --> D[.tex] \n    D --> F((Xetex))\n    C --> I((Hugo))\n    style B fill:#FF6655AA\n    style F fill:#88ffFF\n    style I fill:#88ffFF\nend\n    B ----> E[.doc]\n    B ----> H[.pptx]\n    F --> G[.pdf]\n    I --> J[.html]"
  },
  {
    "objectID": "13_git.html",
    "href": "13_git.html",
    "title": "Git/GitHub",
    "section": "",
    "text": "1 Git/GitHub"
  },
  {
    "objectID": "20_sql.html",
    "href": "20_sql.html",
    "title": "SQL",
    "section": "",
    "text": "1 SQL"
  },
  {
    "objectID": "21_r.html",
    "href": "21_r.html",
    "title": "R",
    "section": "",
    "text": "1 R"
  },
  {
    "objectID": "22_python.html",
    "href": "22_python.html",
    "title": "파이썬",
    "section": "",
    "text": "1 파이썬"
  },
  {
    "objectID": "30_dashboard.html",
    "href": "30_dashboard.html",
    "title": "대쉬보드",
    "section": "",
    "text": "1 대쉬보드 개발 과정\n\n데이터 및 EDA\n모듈\n대쉬보드"
  },
  {
    "objectID": "31_models.html",
    "href": "31_models.html",
    "title": "기계학습",
    "section": "",
    "text": "1 기계학습"
  },
  {
    "objectID": "32_product.html",
    "href": "32_product.html",
    "title": "데이터과학 제품과 서비스",
    "section": "",
    "text": "1 데이터 제품개발"
  },
  {
    "objectID": "41_time_series.html",
    "href": "41_time_series.html",
    "title": "시계열 데이터",
    "section": "",
    "text": "1 시계열 데이터"
  },
  {
    "objectID": "42_network.html",
    "href": "42_network.html",
    "title": "네트워크 데이터",
    "section": "",
    "text": "1 네트워크 데이터"
  },
  {
    "objectID": "43_geospatial.html",
    "href": "43_geospatial.html",
    "title": "공간정보 데이터",
    "section": "",
    "text": "1 공간정보 데이터"
  },
  {
    "objectID": "44_text_mining.html",
    "href": "44_text_mining.html",
    "title": "텍스트 데이터",
    "section": "",
    "text": "1 텍스트 데이터"
  },
  {
    "objectID": "51_nlp.html",
    "href": "51_nlp.html",
    "title": "딥러닝 - NLP",
    "section": "",
    "text": "1 자연어 처리"
  },
  {
    "objectID": "52_vision.html",
    "href": "52_vision.html",
    "title": "딥러닝 - 비전",
    "section": "",
    "text": "1 딥러닝 기본기"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "데이터셋",
    "section": "",
    "text": "gapminder\n\nR 데이터 패키지: gapminder: Data from Gapminder\n\npalmer penguin\n\niris\nR built-in 내장 데이터: iris\n펭귄 vs 붓꽃 데이터\n\nBitData\n\n\n\n\n\nMNIST\n\n\n\n\n\nData Carpentry, 원데이터, weecology/portal-teachingdb\nDVD 대여 데이터베이스: GitHub, PostgreSQL Sample Database, PostgreSQL Sample Database, Load PostgreSQL Sample Database\nSoftware Carpentry: survey.db , DB 설치방법"
  },
  {
    "objectID": "data_chatGPT.html",
    "href": "data_chatGPT.html",
    "title": "chatGPT 데이터셋",
    "section": "",
    "text": "OpenAI에서 최근에 공개한 chatGPT와 함께 인공지능과 공존할 수 있는 방법을 모색해 봅니다.\n\n1 회귀모형\n\n\n\n\n\n\nwhat is the best dataset for regression?\n\n\n\n\n\nThere is no one “best” dataset for regression, as the appropriate dataset will depend on the specific problem you are trying to solve. Some common datasets that are often used for regression include the Boston Housing dataset, the California Housing dataset, and the Diabetes dataset. These datasets are well-studied and can be easily accessed by researchers, making them a good starting point for many regression tasks. Ultimately, the best dataset for your regression problem will depend on the specific problem you are trying to solve and the nature of the data you have available.\n\n\n\n\n\n\n\n\n\nhow about mpg dataset?\n\n\n\n\n\nThe mpg dataset is a dataset that contains information on the fuel efficiency of different cars. It is commonly used in regression tasks, as the goal is often to predict a car’s fuel efficiency based on various features such as its engine size and horsepower. This dataset can be a good choice for regression tasks, as it contains a large number of records and a variety of features that can be used to train a regression model. However, as with any dataset, it’s important to carefully evaluate the quality and relevance of the data before using it for your specific task.\n\n\n\n\n\n\n\n\n\ncan you show me a sample code with R?\n\n\n\n\n\nSure, here is a simple example of a linear regression model in R:\n\n# Load the necessary libraries\nlibrary(tidyverse)\n\n# Load the mpg dataset\ndata(\"mpg\")\n\n# Fit a linear regression model using the \"displ\" and \"hwy\" variables\nmodel <- lm(hwy ~ displ, data = mpg)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n# Predict the hwy fuel efficiency for a car with a displacement of 3\npredict(model, newdata = data.frame(displ = 3))\n\n       1 \n25.10588 \n\n\nThis code fits a linear regression model using the displ and hwy variables from the mpg dataset, and then uses the model to predict the fuel efficiency of a car with a displacement of 3. Note that this is a very simple example, and there are many different ways to perform regression in R. For more information and examples, you can refer to the R documentation and tutorials on regression.\n\n\n\n\n\n2 분류모형\n\n\n\n\n\n\nI am learning machine learning. In particular I’d like to build classification model. Could you recommend a suitable dataset for this purpose?\n\n\n\n\n\nSure, there are many datasets that you can use to build a classification model. One good dataset to start with is the Iris dataset, which is a popular dataset used in machine learning examples. It contains data on the different species of iris flowers, including their petal and sepal length and width. This dataset is small, so it’s easy to work with, but it’s also complex enough to provide a good challenge for a classification model. Other datasets that you might consider include the MNIST dataset for handwritten digit recognition, the CIFAR-10 dataset for object recognition, and the IMDB dataset for sentiment analysis. Ultimately, the best dataset for you to use will depend on your specific goals and needs.\n\n\n\n\n\n\n\n\n\nCan you show me sample python code for building classification machine learning model?\n\n\n\n\n\nHere is an example of how you might build a simple classification model using Python:\n\n# Import the necessary libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the iris dataset\niris = datasets.load_iris()\n\n# Split the data into features and labels\nX = iris.data\ny = iris.target\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nscore = model.score(X_test, y_test)\n\n# Print the accuracy of the model\nprint(\"Accuracy:\", score)\n\nThis code creates a logistic regression model and trains it on the Iris dataset. It then evaluates the model on the test data and prints the model’s accuracy. This is just one example of how you might build a classification model using Python – there are many other algorithms and approaches that you can use.\n\n\n\n\n\n\n\n\n\nCan you translate the python code above into R code with tidymodels?\n\n\n\n\n\nSure, here is an example of how you might translate the Python code above into R code using the tidymodels package:\n\n# Install and load the tidymodels package\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\n\n# Load the iris dataset\niris <- datasets::iris\n\n# Split the data into features and labels\nX <- iris %>% select(-Species)\ny <- iris %>% pull(Species)\n\n# Split the data into training and test sets\nsplits <- initial_split(iris, prop = 0.8)\n\n# Preprocess the data using recipe\nrec <- recipe(Species ~ ., data = iris) %>%\n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# Create a logistic regression model\nlr_mod <- logistic_reg() %>%\n  set_engine(\"glm\") \n\n# Train the model on the training data\nlr_fit <- lr_mod %>%\n  fit(rec, data = training(splits))\n\nError in new.env(parent = parent): use of NULL environment is defunct\n\n# Evaluate the model on the test data\nlr_pred <- lr_fit %>%\n  predict(new_data = testing(splits))\n\nError in predict(., new_data = testing(splits)): object 'lr_fit' not found\n\n# Print the accuracy of the model\nlr_pred %>% metrics(truth = testing(splits) %>% pull(Species), estimate = .pred_class) %>%\n  metric_set(acc)\n\nError in metrics(., truth = testing(splits) %>% pull(Species), estimate = .pred_class): object 'lr_pred' not found\n\n\nThis code uses the tidymodels package to create a logistic regression model and train it on the Iris dataset. It then evaluates the model on the test data and prints the model’s accuracy. This is just one example of how you might build a classification model using tidymodels – there are many other algorithms and approaches that you can use.\n\n\n\n\n\n3 차원축소\n\n\n\n\n\n\nI’d like to practice multivariate data analysis. In particular I am interested in principal component analysis. Can you recommend a dataset for this purpose?\n\n\n\n\n\nSure! There are many publicly available datasets that would be suitable for practicing principal component analysis. One example is the Iris dataset, which is a well-known dataset in the field of statistics and machine learning. It contains 150 samples of iris flowers, with each sample having four measurements: sepal length, sepal width, petal length, and petal width. These measurements are used to classify the iris flowers into three different species: Iris setosa, Iris virginica, and Iris versicolor.\n\n\n\n\n\n\n\n\n\nAlready I used the Iris data. Can you recommend another dataset, something interesting?\n\n\n\n\n\nSure! Here are a few other publicly available datasets that you could use for practicing principal component analysis:\n\nThe Wine dataset, which contains chemical analysis of 13 different wines grown in the same region in Italy. This dataset can be used to classify the wines based on their chemical properties.\nThe MNIST dataset, which contains images of handwritten digits. This dataset can be used to classify the digits based on their visual appearance.\nThe Human Activity Recognition dataset, which contains sensor readings from smartphones worn by 30 different subjects performing six different activities. This dataset can be used to classify the activities based on the sensor readings.\n\nThese are just a few examples - there are many other publicly available datasets that you could use for practicing principal component analysis. You can find many more by searching online for “multivariate datasets” or “principal component analysis datasets.”\n\n\n\n\n\n\n\n\n\nshow me a sample R code for the Wine dataset.\n\n\n\n\n\nHere is a sample R code that you could use to perform principal component analysis on the Wine dataset:\n\n# Load the wine dataset\ndata <- read.csv(\"wine.csv\")\n\n# Perform principal component analysis\npca <- prcomp(data, scale = TRUE)\n\n# Print the results\nprint(pca)\n\n\n\n\n\n\n4 SQL 데이터베이스\n\n\n\n\n\n\nWhat is the best dataset for learning SQL?\n\n\n\n\n\nThere are many datasets that you can use to learn SQL, and the best one for you will depend on your goals and what you want to learn. One option is the Northwind database, which is a small, well-known dataset that is often used for learning SQL. It includes data on customers, orders, and products, and you can use it to practice a wide range of SQL queries. Other options include the AdventureWorks database and the Chinook database, both of which are similar to Northwind and can be used for learning SQL. Ultimately, the best dataset for you to use will depend on your individual learning goals and what you hope to get out of the experience.\n\n\n\n\n\n\n\n\n\nI need license free datasets. which one is the best for learning SQL?\n\n\n\n\n\nIf you are looking for license-free datasets for learning SQL, you have several options. One option is the Northwind database, which is a small, well-known dataset that is often used for learning SQL. It includes data on customers, orders, and products, and you can use it to practice a wide range of SQL queries. Other options include the AdventureWorks database and the Chinook database, both of which are similar to Northwind and can be used for learning SQL. Additionally, there are many other free and open datasets available online that you can use for learning SQL. Some websites that offer free datasets include Kaggle, the UCI Machine Learning Repository, and the US government’s data.gov website.\n\n\n\n\n\n\n\n\n\nIs Northwind dataset really copyright-free?\n\n\n\n\n\nThe Northwind database is not copyright-free. It is copyrighted by Microsoft, and its use is subject to the terms of the Microsoft SQL Server Sample License Agreement. However, Microsoft allows users to download and use the Northwind database for educational and non-commercial purposes, provided that they comply with the terms of the license agreement. If you want to use the Northwind database for commercial purposes, you will need to obtain a commercial license from Microsoft. It is important to always read and understand the terms of any license agreement before using a dataset for any purpose.\n\n\n\n\n\n\n\n\n\nhow about DVD rental dataset?\n\n\n\n\n\nI’m sorry, but I am not able to browse the internet to find information on specific datasets. As a large language model trained by OpenAI, my knowledge is based on the text that I have been trained on, and I do not have the ability to browse the internet or access external information. I can only provide information based on what I have been trained on, which has a fixed cutoff date. In this case, I do not have any information on the DVD rental dataset, as my training data only goes up until 2021 and I do not have the ability to access or browse external information.\n\n\n\n\n\n\n저작권\n데이터베이스\n데이터셋\n설명\n\n\n\n\n상용\nMicrosoft SQL\nNorthwind\nThe Northwind database contains the sales data for a fictitious company called “Northwind Traders,” which imports and exports specialty foods from around the world.\n\n\n상용\nMicrosoft SQL\nAdventureWorks\nOLTP database for a fictitious, multinational manufacturing company called Adventure Works Cycles\n\n\nCC0\nNorthwind 대안\nChinook\ndigital media store\n\n\n상용\nOracle MySQL\nSakila\nMovie rental database with actors, ratings, payments, etc\n\n\nBSD\nPostgreSQL\nPagila\nDVD rental store, containing information about films (like title, category, actresses), rental stores (like address, staff members, customers) and rentals, where a customer rents a film from a store through its staff.\n\n\nMIT\nSQlite3\nsurvey\nEducation DB for SQL lesson of the Software Carpentry\n\n\nMIT\nSQlite3\nPortal Project Teaching\nEducation DB for SQL lesson of the Data Carpentry\n\n\n???\nduckdb\nrelease\ntaxi, job, stock …"
  },
  {
    "objectID": "dataxray.html",
    "href": "dataxray.html",
    "title": "데이터 X-레이",
    "section": "",
    "text": "1 skimr\nskimr 패키지를 사용하여 분석할 데이터와 친숙해진다.\n\nlibrary(tidyverse)\n\nbitData::penguins %>% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n333\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\n종명칭\n0\n1\nFALSE\n3\n아델리: 146, 젠투: 119, 턱끈: 68\n\n\n섬이름\n0\n1\nFALSE\n3\n비스코: 163, 드림: 123, 토르거: 47\n\n\n성별\n0\n1\nFALSE\n2\n수컷: 168, 암컷: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n부리_길이\n0\n1\n43.99\n5.47\n32.1\n39.5\n44.5\n48.6\n59.6\n▃▇▇▆▁\n\n\n부리_깊이\n0\n1\n17.16\n1.97\n13.1\n15.6\n17.3\n18.7\n21.5\n▅▆▇▇▂\n\n\n물갈퀴_길이\n0\n1\n200.97\n14.02\n172.0\n190.0\n197.0\n213.0\n231.0\n▂▇▃▅▃\n\n\n체중\n0\n1\n4207.06\n805.22\n2700.0\n3550.0\n4050.0\n4775.0\n6300.0\n▃▇▅▃▂\n\n\n연도\n0\n1\n2008.04\n0.81\n2007.0\n2007.0\n2008.0\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\n\n2 dataxray\ndataxray 패키지를 사용해서 데이터에 대한 이해를 더욱 높일 수 있다.\n\n\nlibrary(dataxray)\n\nbitData::penguins %>% \n   make_xray() %>% \n   view_xray()\n\n\n\n\nExpand/collapse all\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 Hmisc::describe()\nHmisc 패키지를 통해 과거 20년전 데이터 분석방법을 음미합니다.\n\nHmisc::describe(bitData::penguins)\n\nbitData::penguins \n\n 8  Variables      333  Observations\n--------------------------------------------------------------------------------\n종명칭 \n       n  missing distinct \n     333        0        3 \n                                     \nValue        아델리     턱끈     젠투\nFrequency    146       68      119   \nProportion 0.438    0.204    0.357   \n--------------------------------------------------------------------------------\n섬이름 \n       n  missing distinct \n     333        0        3 \n                                     \nValue        비스코     드림 토르거센\nFrequency    163      123       47   \nProportion 0.489    0.369    0.141   \n--------------------------------------------------------------------------------\n부리_길이 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     333        0      163        1    43.99    6.284     35.7     36.6 \n     .25      .50      .75      .90      .95 \n    39.5     44.5     48.6     50.8     52.0 \n\nlowest : 32.1 33.1 33.5 34.0 34.4, highest: 55.1 55.8 55.9 58.0 59.6\n--------------------------------------------------------------------------------\n부리_깊이 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     333        0       79        1    17.16     2.26    13.90    14.32 \n     .25      .50      .75      .90      .95 \n   15.60    17.30    18.70    19.50    20.00 \n\nlowest : 13.1 13.2 13.3 13.4 13.5, highest: 20.7 20.8 21.1 21.2 21.5\n--------------------------------------------------------------------------------\n물갈퀴_길이 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     333        0       54    0.999      201    15.97      181      185 \n     .25      .50      .75      .90      .95 \n     190      197      213      221      225 \n\nlowest : 172 174 176 178 180, highest: 226 228 229 230 231\n--------------------------------------------------------------------------------\n체중 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     333        0       93        1     4207    915.3     3150     3300 \n     .25      .50      .75      .90      .95 \n    3550     4050     4775     5440     5670 \n\nlowest : 2700 2850 2900 2925 3000, highest: 5850 5950 6000 6050 6300\n--------------------------------------------------------------------------------\n성별 \n       n  missing distinct \n     333        0        2 \n                          \nValue         수컷    암컷\nFrequency    168     165  \nProportion 0.505   0.495  \n--------------------------------------------------------------------------------\n연도 \n       n  missing distinct     Info     Mean      Gmd \n     333        0        3    0.888     2008   0.8857 \n                            \nValue       2007  2008  2009\nFrequency    103   113   117\nProportion 0.309 0.339 0.351\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "들어가며",
    "section": "",
    "text": "1 교육 전후\n체계적인 데이터 과학 교육전에는 실무 업무를 동시에 진행하면서 그때 그때 필요한 도구를 학습하여 적용하는 방식으로 진행된 반면 교육 후에는 다양한 도구를 그때 데이터의 상황과 문제에 맞춰 체계적으로 데이터 과학 제품을 개발하여 가치를 창출하고 유지발전시켜 나가게 됩니다."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "프로젝트",
    "section": "",
    "text": "기획\n분석\n개발\n테스트\n운영\n…"
  },
  {
    "objectID": "02_setup_sql.html",
    "href": "02_setup_sql.html",
    "title": "SQL 환경설정",
    "section": "",
    "text": "SQLite 다운로드 웹사이트에서 운영체제에 적합한 SQlite 소프트웨어를 설치한다.\n\n\n예를 들어, 윈도우 10 환경에서 “Precompiled Binaries for Windows” → sqlite-tools-win32-x86-3400000.zip 파일을 다운로드 받는다.\n다음으로 압축을 풀어 다음 순서로 설치를 완료하고 정상적으로 설치되었는지 확인한다.\n\n\n\n\n\n\nSQlite 다운로드\n\n\n\n\n\nSQlite 설치\n\n\n\n\n\nSqlite 환경등록\n\n\n\n\n\nSQlite 실행과 종료\n\n\n\n그림 1: SQlite 설치 및 헬로월드\n\n\n\n\n\n맥에서 Sqlite를 설치하는 방법은 매우 단순하다. DB Browser for SQLite 웹사이트에서 맥버전(Intell or Apple Silicon) 버전을 다운로드 받아 설치하면 된다.\n\n\n\n\n\n\n ~/swc/curriculum   main ±  sqlite3 --version\n3.37.0 2021-12-09 01:34:53 9ff244ce0739f8ee52a3e9671adb4ee54c83c640b02e3f9d185fd2f9a179aapl\n~/swc/curriculum   main  sqlite3\nSQLite version 3.37.0 2021-12-09 01:34:53\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nsqlite> .quit\n ~/swc/curriculum   main "
  },
  {
    "objectID": "02_setup_sql.html#명령라인",
    "href": "02_setup_sql.html#명령라인",
    "title": "SQL 환경설정",
    "section": "3.1 명령라인",
    "text": "3.1 명령라인\nSQlite가 설치되고 데이터베이스가 있다면 SQL 쿼리문을 작성하여 원하는 결과를 얻을 수 있다.\n\n ~/swc/curriculum   main ±  sqlite3 data/survey.db\nSQLite version 3.37.0 2021-12-09 01:34:53\nEnter \".help\" for usage hints.\nsqlite> .table\nPerson   Site     Survey   Visited\nsqlite> SELECT * from Person;\ndyer|William|Dyer\npb|Frank|Pabodie\nlake|Anderson|Lake\nroe|Valentina|Roerich\ndanforth|Frank|Danforth\nsqlite> .quit"
  },
  {
    "objectID": "02_setup_sql.html#쿼리도구",
    "href": "02_setup_sql.html#쿼리도구",
    "title": "SQL 환경설정",
    "section": "3.2 쿼리도구",
    "text": "3.2 쿼리도구\n동일한 사항을 DB Browser for SQLite 쿼리도구를 사용하면 직관적으로 다양한 SQL 문을 데이터베이스에 던져 원하는 결과를 얻을 수 있다."
  },
  {
    "objectID": "02_setup_sql.html#데이터셋",
    "href": "02_setup_sql.html#데이터셋",
    "title": "SQL 환경설정",
    "section": "4.1 데이터셋",
    "text": "4.1 데이터셋\nNYC Taxi Trip Data - Google Public Data 데이터셋은 구글 빅쿼리(Bigquery) 공개 데이터셋중 일부로 뉴육택시 운행 천만건을 담고 있다. 뉴욕 택시 데이터셋에 대한 자세한 정보는 캐글 웹사이트에서 확인할 수 있다."
  },
  {
    "objectID": "02_setup_sql.html#데이터베이스",
    "href": "02_setup_sql.html#데이터베이스",
    "title": "SQL 환경설정",
    "section": "4.2 데이터베이스",
    "text": "4.2 데이터베이스\nduckdb패키지를 설치하여 taxis.duckdb를 파일로 생성하고 연결을 시켜둔다.\n\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(DBI)\nlibrary(vroom)\nlibrary(tictoc)\n\ndatabase_path <- paste0(here::here(), \"/data/taxis.duckdb\")\nfile.remove(database_path)\ncon <- dbConnect(duckdb(), dbdir = database_path)\ndbListTables(con) \n\ndata\\ 디렉토리 아래 뉴욕 택시 운행 데이터와 duckdb가 하나 파일명으로 taxis.duckdb 생성된 것이 확인된다.\n\nfs::dir_tree(\"data\")\n\ndata\n├── B사감과러브레터.txt\n├── hangul_sosul.txt\n├── original_cleaned_nyc_taxi_data_2018.csv\n├── survey.db\n└── taxis.duckdb"
  },
  {
    "objectID": "02_setup_sql.html#테이블-추가",
    "href": "02_setup_sql.html#테이블-추가",
    "title": "SQL 환경설정",
    "section": "4.3 테이블 추가",
    "text": "4.3 테이블 추가\nduckdb 데이터베이스에 뉴욕택시 데이터셋을 테이블로 추가한다.\n\ntaxis_path <- paste0(here::here(), \"/data/original_cleaned_nyc_taxi_data_2018.csv\")\ntable_create_qry <- glue::glue(\n  \"CREATE TABLE trips AS SELECT * FROM read_csv_auto ('{taxis_path}')\"\n  )\ndbExecute(con, table_create_qry)\n\n[1] 8319928"
  },
  {
    "objectID": "02_setup_sql.html#테이블-확인",
    "href": "02_setup_sql.html#테이블-확인",
    "title": "SQL 환경설정",
    "section": "4.4 테이블 확인",
    "text": "4.4 테이블 확인\ndbListTables() 명령어로 데이터베이스 내 테이블이 제대로 올라갔는지 확인한다.\n\ndbListTables(con)\n\n[1] \"trips\""
  },
  {
    "objectID": "02_setup_sql.html#db-연결-끊기",
    "href": "02_setup_sql.html#db-연결-끊기",
    "title": "SQL 환경설정",
    "section": "4.5 DB 연결 끊기",
    "text": "4.5 DB 연결 끊기\ncon으로 DB에 연결을 했다면 다음으로 연결을 dbDisconnect() 명령어로 연결을 해제한다.\n\ndbDisconnect(con, shutdown=TRUE)"
  },
  {
    "objectID": "02_setup_sql.html#sql-쿼리-1",
    "href": "02_setup_sql.html#sql-쿼리-1",
    "title": "SQL 환경설정",
    "section": "4.6 SQL 쿼리",
    "text": "4.6 SQL 쿼리\n지금까지 작업한 사항내용을 그림으로 요약하면 다음과 같다.\n\n파일 duckDB 데이터베이스를 생성한다.\n데이터베이스에 con DB 핸들러를 통해 R/파이썬 연결을 시킨다.\nCSV 파일을 테이블로 데이터베이스에 올린다.\n정상적으로 테이블이 데이터베이스에 등록되었는지를 확인한다.\nDB 핸들러를 반납하고 연결을 해제시킨다.\n\n\n\n\n\n\n이제부터 본격적으로 OLAP 분석작업을 수행한다. 파일 “/data/taxis.duckdb” 데이터베이스에 DB 핸들러를 연결시킨다. 그리고 나서 분석대상 테이블이 존재하는지 dbListTables() 명령어로 확인한다.\n\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(DBI)\nlibrary(vroom)\nlibrary(tictoc)\n\ndatabase_path <- paste0(here::here(), \"/data/taxis.duckdb\")\n\ncon <-dbConnect(duckdb::duckdb(), dbdir = database_path, read_only=TRUE)\n\ndbListTables(con)\n\n[1] \"trips\"\n\n\ntictock 패키지를 통해 해당 쿼리가 수행되는데 실행된 시간을 측정한다.\n\ntic()\n\nfare_summary <- con |> \n  tbl(\"trips\") |> \n  dplyr::select(payment_type, fare_amount, trip_distance) |> \n  filter(trip_distance > 18) |> \n  group_by(payment_type) |> \n  summarise(average_fare = mean(fare_amount, na.rm = TRUE)) |> \n  collect()\n\ntoc()\n\n0.11 sec elapsed\n\nfare_summary\n\n# A tibble: 4 × 2\n  payment_type average_fare\n         <int>        <dbl>\n1            1         65.4\n2            2         60.9\n3            3         62.5\n4            4         67.1"
  },
  {
    "objectID": "02_setup_sql.html#sql-문",
    "href": "02_setup_sql.html#sql-문",
    "title": "SQL 환경설정",
    "section": "4.7 SQL 문",
    "text": "4.7 SQL 문\ndplyr 데이터 핸들링 문법이 매우 직관적이고 사용하기 편하다. 이를 SQL문으로 변환하는 것도 단순하다. 이를 동일하게 SQL 문으로 작성하여 쿼리문을 던져 분석작업을 수행하자.\n\nlibrary(dbplyr)\n\ntrips_db <- tbl(con, \"trips\")\n\ntrips_sql_query <- trips_db %>% \n  dplyr::select(payment_type, fare_amount, trip_distance) %>% \n  filter(trip_distance > 18) %>% \n  group_by(payment_type) %>% \n  summarise(average_fare = mean(fare_amount, na.rm = TRUE)) %>% \n  show_query()\n\n<SQL>\nSELECT payment_type, AVG(fare_amount) AS average_fare\nFROM (\n  SELECT payment_type, fare_amount, trip_distance\n  FROM trips\n) q01\nWHERE (trip_distance > 18.0)\nGROUP BY payment_type\n\ntrips_sql_query\n\n# Source:   SQL [4 x 2]\n# Database: DuckDB 0.6.1 [statkclee@Windows 10 x64:R 4.2.2/D:/tcs/curriculum/data/taxis.duckdb]\n  payment_type average_fare\n         <int>        <dbl>\n1            1         65.4\n2            2         60.9\n3            3         62.5\n4            4         67.1\n\n\n동일한 결과를 dbGetQuery()함수로 결과값을 얻을 수 있다.\n\nsql_query_from_dbplyr <- \"SELECT payment_type, AVG(fare_amount) AS average_fare\nFROM (\n  SELECT payment_type, fare_amount, trip_distance\n  FROM trips\n) q01\nWHERE (trip_distance > 18.0)\nGROUP BY payment_type\"\n\n# dbGetQuery(con, \"SELECT * FROM trips LIMIT 5;\")\ndbGetQuery(con, sql_query_from_dbplyr)\n\n  payment_type average_fare\n1            1     65.40479\n2            2     60.86640\n3            3     62.52643\n4            4     67.13895"
  },
  {
    "objectID": "02_setup_sql.html#dbeaver-sql-쿼리-도구",
    "href": "02_setup_sql.html#dbeaver-sql-쿼리-도구",
    "title": "SQL 환경설정",
    "section": "4.8 DBeaver SQL 쿼리 도구",
    "text": "4.8 DBeaver SQL 쿼리 도구\nDBeaver Community - Free Universal Database Tool 도구를 다운로드 받고 앞서 구축한 뉴욕 택시 데이터베이스를 연결하면 동일한 결과를 얻을 수 있다.\n먼저, DBeaver Community - Free Universal Database Tool 웹사이트에서 운영체제에 맞는 SQL 쿼리 도구를 설치한다.\n\n\n\n\n\n그리고 나서 앞서 dbplyr show_query() 함수를 사용해서 SQL 문을 복사하여 붙여넣기 하면 해당 결과를 얻을 수 있다."
  },
  {
    "objectID": "02_setup_sql.html#파일-크기",
    "href": "02_setup_sql.html#파일-크기",
    "title": "SQL 환경설정",
    "section": "4.9 파일 크기",
    "text": "4.9 파일 크기\n뉴욕 택시 원본파일 크기를 살펴보자. 이를 위해서 fs패키지 file_info()함수를 사용해서 확인한다.\n\nfs::file_info(\"data/original_cleaned_nyc_taxi_data_2018.csv\") %>% \n  select(path, type, size)\n\n# A tibble: 1 × 3\n  path                                         type         size\n  <fs::path>                                   <fct> <fs::bytes>\n1 data/original_cleaned_nyc_taxi_data_2018.csv file         719M\n\n\nCSV 파일을 duckDB에서 가져왔을 때 데이터베이스 크기를 살펴보자.\n\nfs::file_info(\"data/taxis.duckdb\") %>% \n  select(path, type, size)\n\n# A tibble: 1 × 3\n  path              type         size\n  <fs::path>        <fct> <fs::bytes>\n1 data/taxis.duckdb file         258M"
  },
  {
    "objectID": "01_setup_scenario.html#최신-배쉬-설치",
    "href": "01_setup_scenario.html#최신-배쉬-설치",
    "title": "CLI 개발환경",
    "section": "2.1 최신 배쉬 설치",
    "text": "2.1 최신 배쉬 설치\n명령라인 데이터분석을 위해서는 먼저 다음과 같이 다양한 쉘 중에서 특정 쉘(bash)을 지정하고 이를 기본 쉘로 설정한다. 왜냐하면 경우에 따라서 쉘 명령어가 동작하지 않는 경우가 있기 때문이다.\n$ brew install bash\n$ which -a bash\n$ sudo chsh -s /usr/local/bin/bash\n$ bash --version\nGNU bash, version 5.2.15(1)-release (x86_64-apple-darwin21.6.0)\nCopyright (C) 2022 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n\n\n\nzsh → bash 기본 쉘 설정\n\n\n\n맥에서 zsh가 기본 쉘로 설정된 경우 이를 bash로 상기 명령어로 지정한다.\n$ sudo chsh -s /usr/local/bin/bash"
  },
  {
    "objectID": "01_setup_scenario.html#배쉬툴-설치",
    "href": "01_setup_scenario.html#배쉬툴-설치",
    "title": "CLI 개발환경",
    "section": "2.2 배쉬툴 설치",
    "text": "2.2 배쉬툴 설치\n배쉬를 설정하고 기본 쉘로 설정한 후에 생산성 향상과 즐거운 데이터분석업무를 위해서 필요한 추가 도구를 설정한다. ble.sh와 bash-it이 가장 많이 사용되고 있어 둘 중에서 마음에 드는 것을 설치한다.\n\n\n\n\n\n배쉬 설치 후 첫 실행화면\n\n\n\n배쉬\n\n\n그림 1: 배쉬 도구 설치 전후 비교\n\n\nbash-it을 다운로드 받아 다음 명령어로 설치한다.\n$ git clone --depth=1 https://github.com/Bash-it/bash-it.git ~/.bash_it\n$ ~/.bash_it/install.sh\n$ source ~/.bash_profile"
  },
  {
    "objectID": "03_setup_python.html",
    "href": "03_setup_python.html",
    "title": "파이썬 환경설정",
    "section": "",
    "text": "참조: Riddhiman (Apr 19, 2022), “Getting started with Python using R and reticulate”, R’tichoke\n\n\nR/쿼토 환경에서 파이썬을 사용하는 가장 간단한 방법은 미니콘다를 사용하는 방식이다. 미니콘다(miniconda)는 작고 가볍기 때문에 아나콘다를 기본 기능을 사용할 수 있는 장점도 있다. 추후 필요한 기능에 필요한 패키지를 설치하여 사용한다.\nR에서 이를 가능하게 하는 방식이 reticulate 패키지를 설치한 후 전체 과정을 단순화시킬 수 있다.\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\ninstall_miniconda(path = \"c:/miniconda\", update = TRUE)\n\n\n\n\n미니콘다가 다른 이슈없이 설치되었다면 conda_list() 함수를 사용해서 `r-reticulate``\n\nuse_python(): 파이썬이 설치된 경로.\nuse_virtualenv(): 파이썬 가상환경(virtualenv)이 설치된 경로.\nuse_condaenv(): 콘다 환경이 설치된 경로.\n\n\nconda_list(conda = \"c:/miniconda/_conda.exe\")\n\n            name                                                           python\n1   r-reticulate                     C:\\\\miniconda\\\\envs\\\\r-reticulate/python.exe\n6      miniconda                                         C:\\\\miniconda/python.exe\n콘다환경에서 가상환경을 구축한다. 가상환경의 명칭을 pyenv로 특정한다.\n\nuse_condaenv(condaenv = \"r-reticulate\", conda = \"c:/miniconda/_conda.exe\")\n\nconda_create(envname = \"pyenv\", conda = \"c:/miniconda/_conda.exe\")\n\n\n\n\nreticulate 패키지에 포함된 py_install() 함수로 기계학습과 데이터 과학에 필요한 패키지를 설치한다. 추후 .ipynb 쥬피터 노트북 파이썬 결과물을 쿼토에서 컴파일하는데 필요한 jupyter 패키지도 설치한다.\n\nreticulate::py_install(packages = c(\"pandas\", \"scikit-learn\", \"matplotlib\"))\nreticulate::py_install(packages = c(\"jupyter\"))"
  },
  {
    "objectID": "03_setup_python.html#파이썬-환경구축-1",
    "href": "03_setup_python.html#파이썬-환경구축-1",
    "title": "파이썬 환경설정",
    "section": "2.1 파이썬 환경구축",
    "text": "2.1 파이썬 환경구축\n콘다환경을 구축하여 파이썬 데이터 과학 프로그램을 실행시킬 수 있는 환경을 구축한다.\n\nlibrary(reticulate)\nuse_condaenv(condaenv = \"r-reticulate\", conda = \"c:/miniconda/_conda.exe\")"
  },
  {
    "objectID": "03_setup_python.html#시각화",
    "href": "03_setup_python.html#시각화",
    "title": "파이썬 환경설정",
    "section": "2.2 시각화",
    "text": "2.2 시각화\n고양이와 개에 대한 피쳐(Feature)를 바탕으로 고양이와 개를 분류하는 분류기계학습 모형을 개발해보자. 먼저, 기계학습을 위한 훈련 시험 데이터를 준비하고 시각화를 한다.\n\ntraining_set = {'Dog':[[1,2],[2,3],[3,1]], 'Cat':[[11,20],[14,15],[12,15]]}\ntesting_set = [15,20]\n\n#ploting all data\nimport matplotlib.pyplot as plt\nc = 'x'\nfor data in training_set:\n    print(data)\n    \n    #print(training_set[data])\n    for i in training_set[data]:\n        plt.plot(i[0], i[1], c, color='c')\n    \n    c = 'o'\nplt.show()"
  },
  {
    "objectID": "03_setup_python.html#분류모형",
    "href": "03_setup_python.html#분류모형",
    "title": "파이썬 환경설정",
    "section": "2.3 분류모형",
    "text": "2.3 분류모형\n다양한 기계학습모형이 존재하지만 먼저 KNN 분류모형을 적합시켜 고양이와 개 분류 기계학습모형을 개발한다.\n\n# 기계학습모형 데이터셋 준비\nx = []\ny = []\nfor group in training_set:\n    \n    for features in training_set[group]:\n        x.append(features)\n        y.append(group)\n\n# 기계학습모형 특정\nfrom sklearn import preprocessing, neighbors\n\n# 데이터에 모형 적합\nclf = neighbors.KNeighborsClassifier()\nclf.fit(x, y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()"
  },
  {
    "objectID": "03_setup_python.html#분류모형-성능",
    "href": "03_setup_python.html#분류모형-성능",
    "title": "파이썬 환경설정",
    "section": "2.4 분류모형 성능",
    "text": "2.4 분류모형 성능\n데이터에 적합시킨 모형이 얼마나 고양이와 개를 잘 분류하는지 예측 정확도를 따져보자.\n\n# 예측모형 성능 평가\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\ndog_df = pd.DataFrame.from_dict(training_set['Dog'])\ndog_df['Y'] = \"Dog\"\n\ncat_df = pd.DataFrame.from_dict(training_set['Cat'])\ncat_df['Y'] = \"Cat\"\n\ntrainging_df = pd.concat([dog_df, cat_df])\n\nfeature_df = trainging_df.drop(['Y'], axis=1)\nlabel_df = trainging_df['Y']\n\nY_preds = clf.predict(feature_df)\n\nprint('모형 정확도   : {:.3f}'.format(accuracy_score(label_df, Y_preds)))\n\n모형 정확도   : 1.000"
  },
  {
    "objectID": "03_setup_python.html#분류모형-예측",
    "href": "03_setup_python.html#분류모형-예측",
    "title": "파이썬 환경설정",
    "section": "2.5 분류모형 예측",
    "text": "2.5 분류모형 예측\n실제로 기계학습모형에 사용된 적이 없는 개와 고양이 데이터를 직접 넣어 어떻게 예측하는지 확인해보자.\n\n# 분류모형 예측\nimport numpy as np\ntesting_set = np.array(testing_set)\ntesting_set = testing_set.reshape(1,-1)\n\n# 예측\nprediction = clf.predict(testing_set)\nprint(prediction)\n\n['Cat']"
  },
  {
    "objectID": "hello_world_sklearn.html",
    "href": "hello_world_sklearn.html",
    "title": "데이터 과학",
    "section": "",
    "text": "training_set = {'Dog':[[1,2],[2,3],[3,1]], 'Cat':[[11,20],[14,15],[12,15]]}\ntesting_set = [15,20]\n\n#ploting all data\nimport matplotlib.pyplot as plt\nc = 'x'\nfor data in training_set:\n    print(data)\n    \n    #print(training_set[data])\n    for i in training_set[data]:\n        plt.plot(i[0], i[1], c, color='c')\n    \n    c = 'o'\nplt.show()\n\n# 기계학습모형 데이터셋 준비\nx = []\ny = []\nfor group in training_set:\n    \n    for features in training_set[group]:\n        x.append(features)\n        y.append(group)\n\n# 기계학습모형 특정\nfrom sklearn import preprocessing, neighbors\n\n# 데이터에 모형 적합\nclf = neighbors.KNeighborsClassifier()\nclf.fit(x, y)\n\n# 예측모형 성능 평가\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\ndog_df = pd.DataFrame.from_dict(training_set['Dog'])\ndog_df['Y'] = \"Dog\"\n\ncat_df = pd.DataFrame.from_dict(training_set['Cat'])\ncat_df['Y'] = \"Cat\"\n\ntrainging_df = pd.concat([dog_df, cat_df])\n\nfeature_df = trainging_df.drop(['Y'], axis=1)\nlabel_df = trainging_df['Y']\n\nY_preds = clf.predict(feature_df)\n\nprint('모형 정확도   : {:.3f}'.format(accuracy_score(label_df, Y_preds)))\n\n# 분류모형 예측\nimport numpy as np\ntesting_set = np.array(testing_set)\ntesting_set = testing_set.reshape(1,-1)\n\n# 예측\nprediction = clf.predict(testing_set)\nprint(prediction)\n\nDog\nCat\n\n\n\n\n\n모형 정확도   : 1.000\n['Cat']"
  },
  {
    "objectID": "hello_world_jupyter.html",
    "href": "hello_world_jupyter.html",
    "title": "데이터 과학",
    "section": "",
    "text": "training_set = {'Dog':[[1,2],[2,3],[3,1]], 'Cat':[[11,20],[14,15],[12,15]]}\ntesting_set = [15,20]\n\n#ploting all data\nimport matplotlib.pyplot as plt\nc = 'x'\nfor data in training_set:\n    print(data)\n    \n    #print(training_set[data])\n    for i in training_set[data]:\n        plt.plot(i[0], i[1], c, color='c')\n    \n    c = 'o'\nplt.show()\n\nDog\nCat\n\n\n\n\n\n\n\n\n\n# 기계학습모형 데이터셋 준비\nx = []\ny = []\nfor group in training_set:\n    \n    for features in training_set[group]:\n        x.append(features)\n        y.append(group)\n\n# 기계학습모형 특정\nfrom sklearn import preprocessing, neighbors\n\n# 데이터에 모형 적합\nclf = neighbors.KNeighborsClassifier()\nclf.fit(x, y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\n\n\n\n# 예측모형 성능 평가\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\ndog_df = pd.DataFrame.from_dict(training_set['Dog'])\ndog_df['Y'] = \"Dog\"\n\ncat_df = pd.DataFrame.from_dict(training_set['Cat'])\ncat_df['Y'] = \"Cat\"\n\ntrainging_df = pd.concat([dog_df, cat_df])\n\nfeature_df = trainging_df.drop(['Y'], axis=1)\nlabel_df = trainging_df['Y']\n\nY_preds = clf.predict(feature_df)\n\nprint('모형 정확도   : {:.3f}'.format(accuracy_score(label_df, Y_preds)))\n\n모형 정확도   : 1.000\n\n\n\n\n\n\n# 분류모형 예측\nimport numpy as np\ntesting_set = np.array(testing_set)\ntesting_set = testing_set.reshape(1,-1)\n\n# 예측\nprediction = clf.predict(testing_set)\nprint(prediction)\n\n['Cat']"
  },
  {
    "objectID": "03_setup_python.html#ipynb-.qmd-1",
    "href": "03_setup_python.html#ipynb-.qmd-1",
    "title": "파이썬 환경설정",
    "section": "3.1 .ipynb → .qmd",
    "text": "3.1 .ipynb → .qmd\n개와 고양이 분류 예측모형 개발을 파이썬 쥬피터 노트북으로 개발을 하였다면 이를 쿼토로 변환시켜 _quarto.yml 파일에 등재시킬 수 있는 .qmd 파일로 변환시킨다.\n$ quarto convert hello_world_jupyter.ipynb\nConverted to hello_world_jupyter.qmd"
  },
  {
    "objectID": "03_setup_python.html#qmd-.ipynb",
    "href": "03_setup_python.html#qmd-.ipynb",
    "title": "파이썬 환경설정",
    "section": "3.2 .qmd → .ipynb",
    "text": "3.2 .qmd → .ipynb\n마찬가지로 .qmd 파일을 쥬피터 .ipynb 파일로 변환시킬 경우 동일하게 quarto convert 명령어를 사용한다.\n$ quarto convert hello_world_qmd.qmd\nConverted to hello_world_qmd.ipynb"
  },
  {
    "objectID": "hello_world_qmd.html",
    "href": "hello_world_qmd.html",
    "title": "데이터 과학",
    "section": "",
    "text": "training_set = {'Dog':[[1,2],[2,3],[3,1]], 'Cat':[[11,20],[14,15],[12,15]]}\ntesting_set = [15,20]\n\n#ploting all data\nimport matplotlib.pyplot as plt\nc = 'x'\nfor data in training_set:\n    print(data)\n    \n    #print(training_set[data])\n    for i in training_set[data]:\n        plt.plot(i[0], i[1], c, color='c')\n    \n    c = 'o'\nplt.show()\n\nDog\nCat\n\n\n\n\n\n\n\n\n\n# 기계학습모형 데이터셋 준비\nx = []\ny = []\nfor group in training_set:\n    \n    for features in training_set[group]:\n        x.append(features)\n        y.append(group)\n\n# 기계학습모형 특정\nfrom sklearn import preprocessing, neighbors\n\n# 데이터에 모형 적합\nclf = neighbors.KNeighborsClassifier()\nclf.fit(x, y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\n\n\n\n# 예측모형 성능 평가\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\ndog_df = pd.DataFrame.from_dict(training_set['Dog'])\ndog_df['Y'] = \"Dog\"\n\ncat_df = pd.DataFrame.from_dict(training_set['Cat'])\ncat_df['Y'] = \"Cat\"\n\ntrainging_df = pd.concat([dog_df, cat_df])\n\nfeature_df = trainging_df.drop(['Y'], axis=1)\nlabel_df = trainging_df['Y']\n\nY_preds = clf.predict(feature_df)\n\nprint('모형 정확도   : {:.3f}'.format(accuracy_score(label_df, Y_preds)))\n\n모형 정확도   : 1.000\n\n\n\n\n\n\n# 분류모형 예측\nimport numpy as np\ntesting_set = np.array(testing_set)\ntesting_set = testing_set.reshape(1,-1)\n\n# 예측\nprediction = clf.predict(testing_set)\nprint(prediction)\n\n['Cat']"
  },
  {
    "objectID": "04_setup_r.html",
    "href": "04_setup_r.html",
    "title": "고급 데이터과학: R",
    "section": "",
    "text": "R 다운로드 웹사이트에서 운영체제에 맞는 R 엔진을 다운로드 받아 설치한다."
  },
  {
    "objectID": "01_setup_bash.html",
    "href": "01_setup_bash.html",
    "title": "CLI 개발환경",
    "section": "",
    "text": "윈도우 환경에서 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구.\n\nGit for Windows: https://gitforwindows.org/\n\n\n\n\n윈도우 10/11에서 WSL(Windows Subsystem Linux)을 설치하여 윈도우에서 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구\n\nWindows Subsystem for Linux (WSL) : https://learn.microsoft.com/en-us/windows/wsl/install\n\n\n\n\n격리 가상화 기술을 사용하여 운영체제에 독립적으로 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구\n\n도커: https://www.docker.com/\n\n명령라인 데이터 분석 작업을 수행하기 위한 환경을 개별적으로 구축하기 보다 이미 구축된 도커 환경을 가져와서 준비한다. [@janssens2021data]\n\n도커를 다운로드 받아 설치한다.\n도커허브에서 도커이미지를 받아온다.\n\ndatasciencetoolbox/dsatcl2e\n\n도커를 실행시키고 로컬 디렉토리와 도커 컨테이너 디렉토리를 일치시킨다.\n\n\n\n\n도커 로컬 디렉토리와 컨테이너 디렉토리 마운트 작업\n\n\n$ docker pull datasciencetoolbox/dsatcl2e \n$ docker run --rm -it datasciencetoolbox/dsatcl2e\n$ docker run --rm -it -v \"$(pwd)\":/home/dst/data datasciencetoolbox/dsatcl2e\n\n\n\n데이터 과학에 특화된 클라우드 서비스를 사용하여 운영체제에 독립적으로 유닉스/리눅스 쉘 명령어는 물론 데이터 과학에 필요한 다양한 기능을 지원하는 서비스\n\nPosit Cloud (formerly RStudio Cloud): https://posit.cloud/\n빛에듀: http://bit-edu.iptime.org/rstudio/"
  },
  {
    "objectID": "01_setup_bash.html#최신-배쉬-설치",
    "href": "01_setup_bash.html#최신-배쉬-설치",
    "title": "CLI 개발환경",
    "section": "2.1 최신 배쉬 설치",
    "text": "2.1 최신 배쉬 설치\n명령라인 데이터분석을 위해서는 먼저 다음과 같이 다양한 쉘 중에서 특정 쉘(bash)을 지정하고 이를 기본 쉘로 설정한다. 왜냐하면 경우에 따라서 쉘 명령어가 동작하지 않는 경우가 있기 때문이다.\n$ brew install bash\n$ which -a bash\n$ sudo chsh -s /usr/local/bin/bash\n$ bash --version\nGNU bash, version 5.2.15(1)-release (x86_64-apple-darwin21.6.0)\nCopyright (C) 2022 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n\n\n\nzsh → bash 기본 쉘 설정\n\n\n\n맥에서 zsh가 기본 쉘로 설정된 경우 이를 bash로 상기 명령어로 지정한다.\n$ sudo chsh -s /usr/local/bin/bash"
  },
  {
    "objectID": "01_setup_bash.html#배쉬툴-설치",
    "href": "01_setup_bash.html#배쉬툴-설치",
    "title": "CLI 개발환경",
    "section": "2.2 배쉬툴 설치",
    "text": "2.2 배쉬툴 설치\n배쉬를 설정하고 기본 쉘로 설정한 후에 생산성 향상과 즐거운 데이터분석업무를 위해서 필요한 추가 도구를 설정한다. ble.sh와 bash-it이 가장 많이 사용되고 있어 둘 중에서 마음에 드는 것을 설치한다.\n\n\n\n\n\n배쉬 설치 후 첫 실행화면\n\n\n\n배쉬\n\n\n그림 1: 배쉬 도구 설치 전후 비교\n\n\nbash-it을 다운로드 받아 다음 명령어로 설치한다.\n$ git clone --depth=1 https://github.com/Bash-it/bash-it.git ~/.bash_it\n$ ~/.bash_it/install.sh\n$ source ~/.bash_profile"
  },
  {
    "objectID": "code/ko_markdown.html",
    "href": "code/ko_markdown.html",
    "title": "데이터 과학",
    "section": "",
    "text": "먼저 디지털 글쓰기를 쉽게 누구나 시작할 수 있습니다. 비밀은 마크다운 형식으로 글을 쓰며, R/파이썬 코드를 실행할 수 있고 수학도 가능합니다."
  },
  {
    "objectID": "code/ko_markdown.html#컴파일",
    "href": "code/ko_markdown.html#컴파일",
    "title": "데이터 과학",
    "section": "컴파일",
    "text": "컴파일\n디지털 글쓰기 작성원고는 마크다운 자체 파일(확장자는 md, qmd, rmd, mkd,.markdown,.pandoc`)이다. 마크다운을 뭔가 다른 것으로 변환할 때 대체로 PDF, 마이크로소프트 워드, html 혹은 텍스트 프로세서에서 볼 수 있는 문서형식으로 변환시킬 때 컴파일해서 디지털 글쓰기 최종 저자물을 만들어낸다."
  },
  {
    "objectID": "12_writing.html#쿼토-설치",
    "href": "12_writing.html#쿼토-설치",
    "title": "디지털 글쓰기(보고서)",
    "section": "2.1 쿼토 설치",
    "text": "2.1 쿼토 설치"
  },
  {
    "objectID": "12_writing.html#쿼토-헬로월드",
    "href": "12_writing.html#쿼토-헬로월드",
    "title": "디지털 글쓰기(보고서)",
    "section": "2.2 쿼토 헬로월드",
    "text": "2.2 쿼토 헬로월드"
  },
  {
    "objectID": "12_writing.html#워드문서",
    "href": "12_writing.html#워드문서",
    "title": "디지털 글쓰기(보고서)",
    "section": "2.3 워드문서",
    "text": "2.3 워드문서\n\n\n\n\n\n\n마크다운 파일을 컴파일하여 워드문서 생성\n\n\n\n\n\npandoc -s ko_markdown.md -o ko_markdown.docx"
  },
  {
    "objectID": "12_writing.html#html-문서",
    "href": "12_writing.html#html-문서",
    "title": "디지털 글쓰기(보고서)",
    "section": "2.4 HTML 문서",
    "text": "2.4 HTML 문서\n\n\n\n\n\n\n마크다운 파일을 컴파일하여 HTML 문서 생성\n\n\n\n\n\npandoc -s ko_markdown.md -o ko_markdown.html"
  },
  {
    "objectID": "12_writing.html#pdf-문서",
    "href": "12_writing.html#pdf-문서",
    "title": "디지털 글쓰기(보고서)",
    "section": "2.5 PDF 문서",
    "text": "2.5 PDF 문서\n\n\n\n\n\n\n마크다운 파일을 컴파일하여 PDF 문서 생성\n\n\n\n\n\npandoc -s ko_markdown.md -o ko_markdown.pdf\npandoc -s ko_markdown.md -o ko_markdown.pdf --pdf-engine=xelatex\npandoc -s ko_markdown.md -o ko_markdown.pdf --pdf-engine=xelatex --variable mainfont='Nanum Myeongjo' -V fontsize=11pt -V papersize:\"a4paper\" -V geometry:margin=1in"
  },
  {
    "objectID": "mcq_stat.html",
    "href": "mcq_stat.html",
    "title": "통계와 확률",
    "section": "",
    "text": "통계에서 모든 경우의 수를 포함하는 집합을 일컷는 용어는?\n\n표본 공간(Sample Space)\n포함집합(Super Set)\n포함공간(Super Space)\n표본집합(Sample Set)\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_stat.html#mcq-2번",
    "href": "mcq_stat.html#mcq-2번",
    "title": "통계와 확률",
    "section": "2 MCQ 2번",
    "text": "2 MCQ 2번\n다음 데이터에서 중위수(Median)을 계산하시오.\n데이터: 23, 97, 12, 38, 62\n\n38\n12\n62\n23\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_stat.html#mcq-2번-1",
    "href": "mcq_stat.html#mcq-2번-1",
    "title": "통계와 확률",
    "section": "3 MCQ 2번",
    "text": "3 MCQ 2번\n다음 데이터에서 중위수(Median)을 계산하시오.\n\n데이터: 23, 97, 12, 38, 62\n\n\n38\n12\n62\n23\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_stat.html#mcq-3번",
    "href": "mcq_stat.html#mcq-3번",
    "title": "통계와 확률",
    "section": "4 MCQ 3번",
    "text": "4 MCQ 3번\n동전 던지기를 했을 때 뒷면이 나올 확률은 얼마인가?\n\n1\n0.5\n0.25\n0\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_stat.html#mcq-4번",
    "href": "mcq_stat.html#mcq-4번",
    "title": "통계와 확률",
    "section": "5 MCQ 4번",
    "text": "5 MCQ 4번\n다음 문장 중 참인 것을 모두 고르시오\n\n전체 관측점 집합을 모집단이라고 한다.\n표본은 모집단을 진실되게 대표해야 한다.\n표본은 모집단의 부분집합이다.\n1,2 & 3 모두\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 4\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-5번",
    "href": "mcq_stat.html#mcq-5번",
    "title": "통계와 확률",
    "section": "6 MCQ 5번",
    "text": "6 MCQ 5번\n확률변수 \\(X\\)의 평균 \\(E(X)\\)는 다음 중 무엇을 표현하는가?\n\n평균\n중위수\n표준편차\n분산\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-6번",
    "href": "mcq_stat.html#mcq-6번",
    "title": "통계와 확률",
    "section": "7 MCQ 6번",
    "text": "7 MCQ 6번\nPDF는 다음 중 무엇의 줄임말인가?\n\nPotential Distribution Function\nProbability Distribution Function\nPotential Derivation Function\nProbability Derivation Function\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-7번",
    "href": "mcq_stat.html#mcq-7번",
    "title": "통계와 확률",
    "section": "8 MCQ 7번",
    "text": "8 MCQ 7번\n다음 수식은 무엇을 표현하고 있는가?\n\n\\(Y = \\alpha + \\beta X\\)\n\n\n단순 선형 회귀\n다중 선형 회귀\n이항 분포\n포아송 분포\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-8번",
    "href": "mcq_stat.html#mcq-8번",
    "title": "통계와 확률",
    "section": "9 MCQ 8번",
    "text": "9 MCQ 8번\n다음과 같이 학생 성적이 주어졌을 때, 평균을 구하시오.\n\n90, 85, 89, 93, 100, 78, 86\n\n\n87.8\n88.71\n73.4\n84.2\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-9번",
    "href": "mcq_stat.html#mcq-9번",
    "title": "통계와 확률",
    "section": "10 MCQ 9번",
    "text": "10 MCQ 9번\n다음 분포 중 이산형 분포가 아닌 것은 무엇인가?\n\n베르누이 분포\n이항 분포\n코시 분포\n포아송 분포\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 어려움"
  },
  {
    "objectID": "mcq_stat.html#mcq-10번",
    "href": "mcq_stat.html#mcq-10번",
    "title": "통계와 확률",
    "section": "10 MCQ 10번",
    "text": "10 MCQ 10번\n독립변수를 이르는 다른 명칭은 다음 중 어느 것인가?\n\n반응 변수 (Response Variable)\n설명된 변수 (Explained Variable)\n반응자 (Reactor)\n회귀변수(Regressor)\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 4\n난이도: 어려움"
  },
  {
    "objectID": "mcq_stat.html#mcq-02번",
    "href": "mcq_stat.html#mcq-02번",
    "title": "통계와 확률",
    "section": "2 MCQ 02번",
    "text": "2 MCQ 02번\n다음 데이터에서 중위수(Median)을 계산하시오.\n데이터: 23, 97, 12, 38, 62\n\n38\n12\n62\n23\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_stat.html#mcq-03번",
    "href": "mcq_stat.html#mcq-03번",
    "title": "통계와 확률",
    "section": "3 MCQ 03번",
    "text": "3 MCQ 03번\n동전 던지기를 했을 때 뒷면이 나올 확률은 얼마인가?\n\n1\n0.5\n0.25\n0\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_stat.html#mcq-04번",
    "href": "mcq_stat.html#mcq-04번",
    "title": "통계와 확률",
    "section": "4 MCQ 04번",
    "text": "4 MCQ 04번\n다음 문장 중 참인 것을 모두 고르시오\n\n전체 관측점 집합을 모집단이라고 한다.\n표본은 모집단을 진실되게 대표해야 한다.\n표본은 모집단의 부분집합이다.\n1,2 & 3 모두\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 4\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-05번",
    "href": "mcq_stat.html#mcq-05번",
    "title": "통계와 확률",
    "section": "5 MCQ 05번",
    "text": "5 MCQ 05번\n확률변수 \\(X\\)의 평균 \\(E(X)\\)는 다음 중 무엇을 표현하는가?\n\n평균\n중위수\n표준편차\n분산\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-06번",
    "href": "mcq_stat.html#mcq-06번",
    "title": "통계와 확률",
    "section": "6 MCQ 06번",
    "text": "6 MCQ 06번\nPDF는 다음 중 무엇의 줄임말인가?\n\nPotential Distribution Function\nProbability Distribution Function\nPotential Derivation Function\nProbability Derivation Function\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-07번",
    "href": "mcq_stat.html#mcq-07번",
    "title": "통계와 확률",
    "section": "7 MCQ 07번",
    "text": "7 MCQ 07번\n다음 수식은 무엇을 표현하고 있는가?\n\n\\(Y = \\alpha + \\beta X\\)\n\n\n단순 선형 회귀\n다중 선형 회귀\n이항 분포\n포아송 분포\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-08번",
    "href": "mcq_stat.html#mcq-08번",
    "title": "통계와 확률",
    "section": "8 MCQ 08번",
    "text": "8 MCQ 08번\n다음과 같이 학생 성적이 주어졌을 때, 평균을 구하시오.\n\n90, 85, 89, 93, 100, 78, 86\n\n\n87.8\n88.71\n73.4\n84.2\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 중간"
  },
  {
    "objectID": "mcq_stat.html#mcq-09번",
    "href": "mcq_stat.html#mcq-09번",
    "title": "통계와 확률",
    "section": "9 MCQ 09번",
    "text": "9 MCQ 09번\n다음 분포 중 이산형 분포가 아닌 것은 무엇인가?\n\n베르누이 분포\n이항 분포\n코시 분포\n포아송 분포\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 어려움"
  },
  {
    "objectID": "code/project.html#하위-목적",
    "href": "code/project.html#하위-목적",
    "title": "프로젝트 개요",
    "section": "2.1 하위 목적",
    "text": "2.1 하위 목적\n강조 문서를 보여줍니다."
  },
  {
    "objectID": "mcq_ds.html",
    "href": "mcq_ds.html",
    "title": "데이터 과학",
    "section": "",
    "text": "데이터 과학에서 사용되는 언어는 다음 중 어떤 것인가요?\n\nC\nC++\n자바\n루비\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 4\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_ds.html#mcq-02번",
    "href": "mcq_ds.html#mcq-02번",
    "title": "데이터 과학",
    "section": "2 MCQ 02번",
    "text": "2 MCQ 02번\nEDA는 다음 중 무엇을 줄임말인가?\n\nExplain Data Artifacts\nExploratory Data Analytics\nExploratory Data Analysis\nExplain Data Analysis\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_ds.html#mcq-03번",
    "href": "mcq_ds.html#mcq-03번",
    "title": "데이터 과학",
    "section": "3 MCQ 03번",
    "text": "3 MCQ 03번\n범주형 데이터를 표현하는데 다음 중 적합한 그래프는 무엇인가요?\n\n산점도 그래프\n선 그래프\n막대\n면적 그래프\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 쉬움"
  },
  {
    "objectID": "mcq_ds.html#mcq-04번",
    "href": "mcq_ds.html#mcq-04번",
    "title": "데이터 과학",
    "section": "4 MCQ 04번",
    "text": "4 MCQ 04번\n행(Row)은 데이터의 어느 부분을 표현한 용어인가?\n\n수평\n수직\n대각선\n1,2 & 3 모두\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 1\n난이도: 중간"
  },
  {
    "objectID": "mcq_ds.html#mcq-05번",
    "href": "mcq_ds.html#mcq-05번",
    "title": "데이터 과학",
    "section": "5 MCQ 05번",
    "text": "5 MCQ 05번\n데이터에 대한 시각적 표현을 일컷는 용어는 무엇인가?\n\n데이터셋\n시각화\nEDA\n데이터 표현\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 중간"
  },
  {
    "objectID": "mcq_ds.html#mcq-06번",
    "href": "mcq_ds.html#mcq-06번",
    "title": "데이터 과학",
    "section": "6 MCQ 06번",
    "text": "6 MCQ 06번\n다음 중 R을 실행하는 방법은 무엇인가?\n\nOS\n디스크 OS\nCLI\n사용자 인터페이스 OS\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 중간"
  },
  {
    "objectID": "mcq_ds.html#mcq-07번",
    "href": "mcq_ds.html#mcq-07번",
    "title": "데이터 과학",
    "section": "7 MCQ 07번",
    "text": "7 MCQ 07번\n다음 중 기계학습 알고리즘이 아닌 것은 무엇인가?\n\nSVM\n회귀모형\n나이브 베이즈\nSVG\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 4\n난이도: 중간"
  },
  {
    "objectID": "mcq_ds.html#mcq-08번",
    "href": "mcq_ds.html#mcq-08번",
    "title": "데이터 과학",
    "section": "8 MCQ 08번",
    "text": "8 MCQ 08번\n다음 중 데이터 과학 작업흐름을 가장 잘 표현한 것은 무엇인가?\n\n의사소통 → 데이터 가져오기 → 데이터 정제 → EDA → 시각화 → 모형\n데이터 가져오기 → 의사소통 → EDA → 시각화 → 모형 → 데이터 정제\n데이터 가져오기 → 데이터 정제 → EDA → 시각화 → 모형 → 의사소통\n데이터 가져오기 → 시각화 → 모형 → 데이터 정제 → 의사소통 → EDA\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 중간"
  },
  {
    "objectID": "mcq_ds.html#mcq-09번",
    "href": "mcq_ds.html#mcq-09번",
    "title": "데이터 과학",
    "section": "9 MCQ 09번",
    "text": "9 MCQ 09번\n통계적/분석적 도구를 사용하여 데이터를 평가하는 방법을 일컷는 용어는 무엇인가?\n\n데이터 탐색\n데이터 분석\n데이터 시각화\n데이터 마이닝\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 2\n난이도: 어려움"
  },
  {
    "objectID": "mcq_ds.html#mcq-10번",
    "href": "mcq_ds.html#mcq-10번",
    "title": "데이터 과학",
    "section": "10 MCQ 10번",
    "text": "10 MCQ 10번\n다음 중 맞는 것을 모두 고르시오?\n\n원데이터는 데이터 정제작업 후에 얻어진다.\n실제 원천 데이터는 처리된 데이터다.\n원데이터가 실제 원천 데이터다.\n2번과 3번\n\n\n\n\n\n\n\n정답\n\n\n\n\n\n\n정답: 3\n난이도: 어려움"
  },
  {
    "objectID": "04_setup_r.html#로컬-글꼴---no-font-name",
    "href": "04_setup_r.html#로컬-글꼴---no-font-name",
    "title": "고급 데이터과학: R",
    "section": "5.1 로컬 글꼴 - “No Font Name”",
    "text": "5.1 로컬 글꼴 - “No Font Name”\n로컬 컴퓨터에 설치를 했으나 제대로 “No Font Name” 이슈가 있는 경우 다음과 같이 해결한다. 즉, Rttf2pt1 이전 버전을 사용하여 운영체제에 등록된 폰트를 R에서 사용할 수 있는 폰트로 등록한다. 그리고 나서, loadfonts() 함수를 사용해서 글꼴을 시각화에서 사용할 수 있도록 조치를 취한다.\n문제해결: How can I resolve the “No Font Name” issue when importing fonts into R using extrafont?, Dataholic, “추가 폰트 사용하기”\n\nlibrary(extrafont)\nlibrary(remotes)\n\nremotes::install_version(\"Rttf2pt1\", version = \"1.3.8\")\nfont_import(paths=\"C:/Windows/Fonts\", prompt=FALSE) # 한번만 실행한다.\n\nloadfonts(device = \"win\", quiet = TRUE)"
  },
  {
    "objectID": "timer_five.html",
    "href": "timer_five.html",
    "title": "모래시계",
    "section": "",
    "text": "−+\n05:00\n\n\n\n\n\n#| echo: false\nlibrary(countdown)\ncountdown::countdown(minutes = 60, seconds = 00, \n          left = 0, right = 0,\n          bottom = 1,\n          padding = \"50px\",\n          margin = \"5%\",\n          font_size = \"10em\")\n#| echo: false\nlibrary(countdown)\ncountdown::countdown(minutes = 10, seconds = 00, \n          left = 0, right = 0,\n          bottom = 1,\n          padding = \"50px\",\n          margin = \"5%\",\n          font_size = \"10em\")\n#| echo: false\nlibrary(countdown)\ncountdown::countdown(minutes = 30, seconds = 00, \n          left = 0, right = 0,\n          bottom = 1,\n          padding = \"50px\",\n          margin = \"5%\",\n          font_size = \"10em\")\n#| echo: false\nlibrary(countdown)\ncountdown::countdown(minutes = 20, seconds = 00, \n          left = 0, right = 0,\n          bottom = 1,\n          padding = \"50px\",\n          margin = \"5%\",\n          font_size = \"10em\")"
  },
  {
    "objectID": "timer_ten.html",
    "href": "timer_ten.html",
    "title": "모래시계",
    "section": "",
    "text": "−+\n10:00"
  },
  {
    "objectID": "timer_hour.html",
    "href": "timer_hour.html",
    "title": "모래시계",
    "section": "",
    "text": "−+\n60:00"
  },
  {
    "objectID": "timer_thirty.html",
    "href": "timer_thirty.html",
    "title": "모래시계",
    "section": "",
    "text": "−+\n30:00"
  },
  {
    "objectID": "timer_twenty.html",
    "href": "timer_twenty.html",
    "title": "모래시계",
    "section": "",
    "text": "−+\n20:00"
  },
  {
    "objectID": "99_statistics.html",
    "href": "99_statistics.html",
    "title": "통계학",
    "section": "",
    "text": "1 통계 강의 슬라이드\n\n쥬피터 노트북\n\n단순회귀모형(OLS)"
  },
  {
    "objectID": "curriculum_2023.html",
    "href": "curriculum_2023.html",
    "title": "2022년 교과과정",
    "section": "",
    "text": "1 학습 내용\n\n\n\n\n\n\n  \n    \n      데이터 과학 교육과정\n\n    \n    \n  \n  \n    \n      과정상세\n      데이터셋\n    \n  \n  \n    \n      1주차 - Introduction\n    \n    - Defining and solving problems\n\n    - Quantifying performance indicators\n\n    - Stats and Probability for Data Science\n\n    \n      2주차 - Data Science\n    \n    - Python 기초 문법 복습 및 NumPy, Pandas 등 연습\n\n    - Kaggle: MPG / Titanic - Machine Learning from Disaster를 이용한 EDA의 개념과 Data science의 전체적인 Flow를 습득\n\n    - EDA, Feature engineering, Visualization, Correlating 등 Preprocessing 작업 수행\n\n    - Model, predict and evaluation 의 전체 과정을 습득\n\n    \n      3주차 - Regression and Prediction\n    \n    - Regression에 대한 개념 및 이론 설명\nBreast Cancer, Boston housing, California housing prices, Bike-sharing-demand etc.\n    - EDA 및 preprocessing (Impute missing value, Turn categorical into booleans, scale the numerical variables, merge etc.)\n\n    - Linear Regression을 중심으로 설명하되, 추가적으로 CART, SVM, Ensemble기법 등을 적용\n\n    - Training & Evaluation (MSE, MAE, RMSE, RMSLE, R-Squared, etc.)\n\n    - Basic hyperparameter tuning 수행\n\n    \n      4주차 - Classification and Hypothesis Testing\n    \n    - Classification과 필요 개념 및 이론 설명\nTitanic, Iris, Penuins, San Francisco Crime etc.\n    - EDA & preprocessing\n\n    - Logistic Regression과 Decision Tree를 중심으로 설명하되, 추가적으로 SVC, Random Forest, XGBoost, LGBM 등 적용\n\n    - Training & Evaluation (Accuracy, Precision, Recall, F1-score, Confusion Matrix, Cross-validation, etc.)\n\n    - Basic hyperparameter tuning 수행\n\n    \n      5주차 - Case Studies and Projects\n    \n    - 기존 수업에서 다루었던 이론/실습 개념을 바탕으로 한 Case Study 3종 (+Final Quiz)\n\n    - Case Study 및 Final Quiz 해설\n\n    - 개인 별 Project 인터뷰 및 Grouping / 공통과제 선정\n\n    - Project 진행, 발표, Best Project 선정 및 공유"
  },
  {
    "objectID": "curriculum_2022.html",
    "href": "curriculum_2022.html",
    "title": "2022년 교과과정",
    "section": "",
    "text": "1 학습 내용\nJanssens (2021), Wickham & Grolemund (2016), Horst et al. (2020), Xie et al. (2020), Severance (2013), Wilson (2006), 한국R사용자회 (2022)\n\n\n\n\n\n\n  \n    \n      데이터 과학 교육과정\n    \n    \n  \n  \n    \n      일정\n      과목명\n      학습상세\n      데이터셋\n    \n  \n  \n    00주차\n환경설정\n학습 도구 설치 및 학습환경\n파머 펭귄, BitData\n    01주차\n디지털 글쓰기(보고서)\n마크다운 / Quarto\nBitData\n    02주차\n명령라인 데이터 분석\n자동화(Shell)\nSWC\n    03주차\n버전 제어\n버전제어/협업(Git / GitHub / GitLab)\n\n    04주차\n프로그래밍\nSQL\nData Carpentry, DVD 렌탈\n    05주차\n프로그래밍\n시각화\ngapminder\n    06주차\n프로그래밍\n파이썬\n\n    07주차\n대쉬보드\nFlexdashboard / Shinydashboard\n\n    08주차\n기계학습\ntidymodels / scikit-learn\n\n    09주차\n데이터 과학 제품\nRESTful API\n\n    10주차\n특수 데이터\n시계열(Time Series)\nKOSPI\n    11주차\n특수 데이터\n공간정보(Geospatial) / 텍스트 분석\n미디어오늘\n    12주차\n비정형 데이터\n텍스트와 이미지 (딥러닝)\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n참고문헌\n\nHorst, A. M., Hill, A. P., & Gorman, K. B. (2020). Palmerpenguins: Palmer archipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/\n\n\nJanssens, J. (2021). Data science at the command line. \" O’Reilly Media, Inc.\".\n\n\nSeverance, C. (2013). Python for informatics: Exploring information. CreateSpace.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. \" O’Reilly Media, Inc.\".\n\n\nWilson, G. (2006). Software carpentry: Getting scientists to write better code by making them more productive. Computing in Science & Engineering, 8(6), 66–69.\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman; Hall/CRC.\n\n\n한국R사용자회. (2022). R 텍스트마이닝. https://r2bit.com/book_tm/"
  },
  {
    "objectID": "py_tree.html",
    "href": "py_tree.html",
    "title": "기계학습",
    "section": "",
    "text": "캐글: Breast Cancer Wisconsin (Diagnostic) Data Set\n여성 가슴에서 미세한 바늘 흡인(FNA)로 추출한 덩어리를 디지털화된 이미지로부터 추출한 정보를 바탕으로 유방암 여부를 예측한다.\n범주 구분: 357 정상(benign), 212 유방암(malignant)"
  },
  {
    "objectID": "py_tree.html#recommend-three-tabular-machine-learning-dataset-for-classification-problem",
    "href": "py_tree.html#recommend-three-tabular-machine-learning-dataset-for-classification-problem",
    "title": "기계학습",
    "section": "4.1 recommend three tabular machine learning dataset for classification problem",
    "text": "4.1 recommend three tabular machine learning dataset for classification problem\n\n\n\n\n\n\n주의\n\n\n\n\n\nSure, here are three tabular machine learning datasets for classification problems:\n\nTitanic: The Titanic dataset is a well-known dataset that contains information on the passengers of the Titanic. The task is to predict whether a passenger survived or not based on features such as their age, gender, and passenger class.\nCredit Card Fraud Detection: This dataset contains credit card transaction data, and the task is to classify transactions as fraudulent or genuine. The dataset contains a large number of features, including the amount of the transaction, time of the transaction, and information about the card used.\nAdult Census Income: This dataset contains information on the income of individuals based on features such as their age, education, and occupation. The task is to predict whether an individual has an income greater than or equal to $50,000 per year.\n\nI hope these datasets are helpful! Let me know if you have any other questions."
  },
  {
    "objectID": "py_tree.html#where-can-i-download-credit-card-fraud-deteciton-dataset-and-adult-census-income-dateset",
    "href": "py_tree.html#where-can-i-download-credit-card-fraud-deteciton-dataset-and-adult-census-income-dateset",
    "title": "기계학습",
    "section": "4.2 where can i download credit card fraud deteciton dataset and adult census income dateset?",
    "text": "4.2 where can i download credit card fraud deteciton dataset and adult census income dateset?\n\n\n\n\n\n\n주의\n\n\n\n\n\nThe Titanic dataset is also available on Kaggle at the following link: https://www.kaggle.com/c/titanic\nThe Credit Card Fraud Detection dataset is available on Kaggle at the following link: https://www.kaggle.com/mlg-ulb/creditcardfraud\nThe Adult Census Income dataset is available on the UCI Machine Learning Repository at the following link: https://archive.ics.uci.edu/ml/datasets/Adult\nThese sites are well-known repositories for machine learning datasets and are a good resource for finding datasets for a variety of tasks.\nI hope this helps! Let me know if you have any other questions."
  },
  {
    "objectID": "py_tree.html#유방암-분류",
    "href": "py_tree.html#유방암-분류",
    "title": "기계학습",
    "section": "2.1 유방암 (분류)",
    "text": "2.1 유방암 (분류)\n여성 가슴에서 미세한 바늘 흡인(FNA)로 추출한 덩어리를 디지털화된 이미지로부터 역산하여 계산한 피쳐를 담고 있고, 각 변수는 존재하는 세포핵의 특성을 나타낸다.\n\n변수 설명\n\n\nID number\n\n\nDiagnosis (M = malignant, B = benign)\n\n\nTen real-valued features are computed for each cell nucleus:\n\n\nradius (mean of distances from center to points on the perimeter)\n\n\ntexture (standard deviation of gray-scale values)\n\n\nperimeter\n\n\narea\n\n\nsmoothness (local variation in radius lengths)\n\n\ncompactness (perimeter^2 / area - 1.0)\n\n\nconcavity (severity of concave portions of the contour)\n\n\nconcave points (number of concave portions of the contour)\n\n\nsymmetry\n\n\nfractal dimension (“coastline approximation” - 1)\n\n\nfield 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n568\n\n\nNumber of columns\n33\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nlogical\n1\n\n\nnumeric\n31\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndiagnosis\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\n…33\n568\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n30425139.67\n125124311.81\n8670.00\n869222.50\n906157.00\n8825022.25\n911320502.00\n▇▁▁▁▁\n\n\nradius_mean\n0\n1\n14.14\n3.52\n6.98\n11.71\n13.38\n15.80\n28.11\n▂▇▃▁▁\n\n\ntexture_mean\n0\n1\n19.28\n4.30\n9.71\n16.17\n18.84\n21.78\n39.28\n▃▇▃▁▁\n\n\nperimeter_mean\n0\n1\n92.05\n24.25\n43.79\n75.20\n86.29\n104.15\n188.50\n▃▇▃▁▁\n\n\narea_mean\n0\n1\n655.72\n351.66\n143.50\n420.30\n551.40\n784.15\n2501.00\n▇▃▂▁▁\n\n\nsmoothness_mean\n0\n1\n0.10\n0.01\n0.06\n0.09\n0.10\n0.11\n0.16\n▂▇▅▁▁\n\n\ncompactness_mean\n0\n1\n0.10\n0.05\n0.02\n0.07\n0.09\n0.13\n0.35\n▇▇▂▁▁\n\n\nconcavity_mean\n0\n1\n0.09\n0.08\n0.00\n0.03\n0.06\n0.13\n0.43\n▇▃▂▁▁\n\n\nconcave points_mean\n0\n1\n0.05\n0.04\n0.00\n0.02\n0.03\n0.07\n0.20\n▇▃▂▁▁\n\n\nsymmetry_mean\n0\n1\n0.18\n0.03\n0.11\n0.16\n0.18\n0.20\n0.30\n▁▇▅▁▁\n\n\nfractal_dimension_mean\n0\n1\n0.06\n0.01\n0.05\n0.06\n0.06\n0.07\n0.10\n▆▇▂▁▁\n\n\nradius_se\n0\n1\n0.41\n0.28\n0.11\n0.23\n0.32\n0.48\n2.87\n▇▁▁▁▁\n\n\ntexture_se\n0\n1\n1.22\n0.55\n0.36\n0.83\n1.11\n1.47\n4.88\n▇▅▁▁▁\n\n\nperimeter_se\n0\n1\n2.87\n2.02\n0.76\n1.61\n2.29\n3.36\n21.98\n▇▁▁▁▁\n\n\narea_se\n0\n1\n40.37\n45.52\n6.80\n17.85\n24.57\n45.24\n542.20\n▇▁▁▁▁\n\n\nsmoothness_se\n0\n1\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.03\n▇▃▁▁▁\n\n\ncompactness_se\n0\n1\n0.03\n0.02\n0.00\n0.01\n0.02\n0.03\n0.14\n▇▃▁▁▁\n\n\nconcavity_se\n0\n1\n0.03\n0.03\n0.00\n0.02\n0.03\n0.04\n0.40\n▇▁▁▁▁\n\n\nconcave points_se\n0\n1\n0.01\n0.01\n0.00\n0.01\n0.01\n0.01\n0.05\n▇▇▁▁▁\n\n\nsymmetry_se\n0\n1\n0.02\n0.01\n0.01\n0.02\n0.02\n0.02\n0.08\n▇▃▁▁▁\n\n\nfractal_dimension_se\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n▇▁▁▁▁\n\n\nradius_worst\n0\n1\n16.28\n4.83\n7.93\n13.02\n14.97\n18.79\n36.04\n▆▇▃▁▁\n\n\ntexture_worst\n0\n1\n25.67\n6.15\n12.02\n21.08\n25.41\n29.68\n49.54\n▃▇▆▁▁\n\n\nperimeter_worst\n0\n1\n107.35\n33.57\n50.41\n84.15\n97.66\n125.53\n251.20\n▇▇▃▁▁\n\n\narea_worst\n0\n1\n881.66\n569.28\n185.20\n515.68\n686.55\n1085.00\n4254.00\n▇▂▁▁▁\n\n\nsmoothness_worst\n0\n1\n0.13\n0.02\n0.07\n0.12\n0.13\n0.15\n0.22\n▂▇▇▂▁\n\n\ncompactness_worst\n0\n1\n0.25\n0.16\n0.03\n0.15\n0.21\n0.34\n1.06\n▇▅▁▁▁\n\n\nconcavity_worst\n0\n1\n0.27\n0.21\n0.00\n0.12\n0.23\n0.38\n1.25\n▇▅▂▁▁\n\n\nconcave points_worst\n0\n1\n0.11\n0.07\n0.00\n0.06\n0.10\n0.16\n0.29\n▅▇▅▃▁\n\n\nsymmetry_worst\n0\n1\n0.29\n0.06\n0.16\n0.25\n0.28\n0.32\n0.66\n▅▇▁▁▁\n\n\nfractal_dimension_worst\n0\n1\n0.08\n0.02\n0.06\n0.07\n0.08\n0.09\n0.21\n▇▃▁▁▁"
  },
  {
    "objectID": "py_tree.html#연비-예측",
    "href": "py_tree.html#연비-예측",
    "title": "기계학습",
    "section": "2.2 연비 (예측)",
    "text": "2.2 연비 (예측)\n데이터 출처: 캐글 자동차 연비\n\n변수 설명\n\nmpg — Mileage/Miles Per Gallon\ncylinders — the power unit of the car where gasoline is turned into power\ndisplacement — engine displacement of the car\nhorsepower — rate of the engine performance\nweight — the weight of a car\nacceleration — the acceleration of a car\nmodel — model of the car\norigin — the origin of the car\ncar — the name of the car\n\n\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n398\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhorsepower\n0\n1\n1\n3\n0\n94\n0\n\n\ncar name\n0\n1\n6\n36\n0\n305\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n23.51\n7.82\n9\n17.50\n23.0\n29.00\n46.6\n▆▇▆▃▁\n\n\ncylinders\n0\n1\n5.45\n1.70\n3\n4.00\n4.0\n8.00\n8.0\n▇▁▃▁▃\n\n\ndisplacement\n0\n1\n193.43\n104.27\n68\n104.25\n148.5\n262.00\n455.0\n▇▂▂▃▁\n\n\nweight\n0\n1\n2970.42\n846.84\n1613\n2223.75\n2803.5\n3608.00\n5140.0\n▇▇▅▅▂\n\n\nacceleration\n0\n1\n15.57\n2.76\n8\n13.83\n15.5\n17.17\n24.8\n▁▆▇▃▁\n\n\nmodel year\n0\n1\n76.01\n3.70\n70\n73.00\n76.0\n79.00\n82.0\n▇▆▇▆▇\n\n\norigin\n0\n1\n1.57\n0.80\n1\n1.00\n1.0\n2.00\n3.0\n▇▁▂▁▂"
  }
]