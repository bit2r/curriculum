[
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "환경설정",
    "section": "",
    "text": "Visual Studio Code: https://code.visualstudio.com/download\nRStudio: https://posit.co/download/rstudio-desktop/\nJupyter: https://jupyter.org/install\n\n\n\n\n\nGit for Windows: https://gitforwindows.org/\n\n\n\n\n\nR: https://cran.r-project.org/bin/windows/base/\n파이썬: https://www.python.org/downloads/\n\n\n\n\n\nQuarto: https://quarto.org/docs/download/\n\n\n\n\n\nSqlite3: https://www.sqlite.org/index.html\nDuckDB: https://duckdb.org/\nPostgreSQL: https://www.postgresql.org/download/\n\n\n\n\n\n텐서플로우: https://www.tensorflow.org/install\n파이토치: https://pytorch.org/\nHuggingface: https://huggingface.co/"
  },
  {
    "objectID": "00_setup.html#전자책",
    "href": "00_setup.html#전자책",
    "title": "환경설정",
    "section": "3.1 전자책",
    "text": "3.1 전자책\n\nBitBook: https://r2bit.com/book"
  },
  {
    "objectID": "00_setup.html#블로그",
    "href": "00_setup.html#블로그",
    "title": "환경설정",
    "section": "3.2 블로그",
    "text": "3.2 블로그\n\n데이터 과학: https://statkclee.github.io/data-science/\n시각화: https://statkclee.github.io/viz/\n문서제작: http://aispiration.com/comp_document/\n\n쿼토(Quarto): https://r2bit.com/quarto/\n파워포인트(PPT): https://r2bit.com/bitSlide/\n웹슬라이드(Xaringan): http://aispiration.com/ds-authoring/\n\n데이터 과학 제품: http://aispiration.com/data-product/\n딥러닝 기본기 : https://aispiration.com/united-states/\n…"
  },
  {
    "objectID": "01_setup_scenario.html",
    "href": "01_setup_scenario.html",
    "title": "개발환경 설정",
    "section": "",
    "text": "1 Git Bash\n윈도우 환경에서 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구.\n\nGit for Windows: https://gitforwindows.org/\n\n\n\n2 WSL\n윈도우 10/11에서 WSL(Windows Subsystem Linux)을 설치하여 윈도우에서 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구\n\nWindows Subsystem for Linux (WSL) : https://learn.microsoft.com/en-us/windows/wsl/install\n\n\n\n3 도커\n격리 가상화 기술을 사용하여 운영체제에 독립적으로 유닉스/리눅스 쉘 명령어를 사용하여 컴퓨터를 조작할 수 있도록 해주는 도구\n\n도커: https://www.docker.com/\n\n\n\n4 클라우드\n데이터 과학에 특화된 클라우드 서비스를 사용하여 운영체제에 독립적으로 유닉스/리눅스 쉘 명령어는 물론 데이터 과학에 필요한 다양한 기능을 지원하는 서비스\n\nPosit Cloud (formerly RStudio Cloud): https://posit.cloud/\n빛에듀: http://bit-edu.iptime.org/rstudio/"
  },
  {
    "objectID": "10_cmdline.html",
    "href": "10_cmdline.html",
    "title": "명령라인 데이터 분석",
    "section": "",
    "text": "저작권 걱정없이 저작물을 받을 수 있는 경로는 여러 곳이 존재한다. 하지만, PDF, HWP, TXT 파일은 압축하여 제공하고 있어 사람손이 몇번씩 가는 문제점이 있다. 작업과정에 추가로 프로세스를 넣어주어야만 된다. 어차치 TXT로 작업하는데 …\n대표적 국내외 공유 저작물 저장소 1 2로 다음을 꼽을 수 있다.\n\n미국: 구텐베르크(Gutenberg) 프로젝트\n일본: 일본판 구텐베르크, 아오조라 문고(靑空文庫, あおぞらぶんこ)\n대한민국\n\n공공누리 포털\n직지(http://www.jikji.org/)\n공유마당"
  },
  {
    "objectID": "10_cmdline.html#데이터-가져오기",
    "href": "10_cmdline.html#데이터-가져오기",
    "title": "명령라인 데이터 분석",
    "section": "3.1 데이터 가져오기",
    "text": "3.1 데이터 가져오기\n데이터를 가져오는 방식은 결국 텍스트로 유닉스/리눅스 환경으로 불러와야만 된다. csvkit 에 in2csv, csvcut, csvlook, sql2csv, csvsql이 포함되어 있다.\nsudo pip install csvkit 명령어로 설치한다.\n\n로컬 파일: cp 복사, 원격파일 복사: scp 복사\n압축파일: tar, unzip, unrar 명령어로 압축된 파일을 푼다.\n\n압축파일 확장자: .tar.gz, .zip, .rar\n압축파일 푸는 종결자 unpack\n\n스프레드쉬트: in2csv는 표형식 엑셀 데이터를 받아 csv 파일로 변환.\n\n$ in2csv ne_1033_data.xlsx | csvcut -c county,item_name,quantity | csvlook | head\n\n데이터베이스: sql2csv\n\nsql2csv --db 'sqlite:///iris.db' --query 'SELECT * FROM iris where petal_length > 6.5' | csvlook\n\n인터넷: curl을 활용하여 인터넷 자원을 긁어온다.\n\ncurl -s http://www.gutenberg.org/files/13693/13693-t/13693-t.tex -o number-theory.txt\n\n\nAPI: curl 물론, API 토큰, 비밀키 등을 설정하거나 일일 이용한도가 있을 수도 있다. 특히, curlicue를 활용하여 트위터 데이터를 바로 가져와서 활용할 수 있다. 자세한 사항은 Create Your Own Dataset Consuming Twitter API 블로그를 참조한다.\n\nRANDOM USER GENERATOR, curl -s http://api.randomuser.me | jq '.'"
  },
  {
    "objectID": "10_cmdline.html#데이터-정제",
    "href": "10_cmdline.html#데이터-정제",
    "title": "명령라인 데이터 분석",
    "section": "3.2 데이터 정제",
    "text": "3.2 데이터 정제\n\n3.2.1 행 뽑아내기\n\n행 위치정보를 기반으로 해서 행 절대번호를 활용하여 추출한다.\n\nhead, sed, awk\n\n패턴을 주고 연관된 행만 추출한다.\n\ngrep 명령어에 정규표현식으로 패턴을 담아 매칭되는 것만 뽑아낸다.\n사용례: grep -i session paper.txt\n\n무작위로 행을 추출한다.\n\nshuf 명령어를 사용한다.\n사용례: shuf -n 10 data.csv\n\n\n\n\n3.2.2 값 추출\n기본적인 값추출 전략은 grep 명령어로 행을 뽑아내고, cut 명령어로 구분자를 두거나 고정된 열위치에 해당하는 열에서 값을 추출한다. cut 명령어로 열을 쪼개는데 구분자로 ,를 사용하고 뽑아내는 열로 -f 인자를 두고 3번째 행이후 모두를 지정한다.\n\n$ grep -i session paper.txt | cut -d ',' -f3-\n$ grep -i session paper.txt | cut -c 7-\n\n\n\n3.2.3 값 바꾸기\n값을 바꾸거나 삭제할 때 사용하는 명령어가 tr로 translate 번역의 약자다.\n공백 을 *로 바꾼다.\n$ echo 'We Love Data Science!' | tr ' ' '*'\nWe*Love*Data*Science!"
  },
  {
    "objectID": "11_shell.html",
    "href": "11_shell.html",
    "title": "유닉스 쉘(Shell)",
    "section": "",
    "text": "1 유닉스 쉘"
  },
  {
    "objectID": "12_writing.html",
    "href": "12_writing.html",
    "title": "디지털 글쓰기(보고서)",
    "section": "",
    "text": "1 디지털 글쓰기"
  },
  {
    "objectID": "13_git.html",
    "href": "13_git.html",
    "title": "Git/GitHub",
    "section": "",
    "text": "1 Git/GitHub"
  },
  {
    "objectID": "20_sql.html",
    "href": "20_sql.html",
    "title": "SQL",
    "section": "",
    "text": "1 SQL"
  },
  {
    "objectID": "21_r.html",
    "href": "21_r.html",
    "title": "R",
    "section": "",
    "text": "1 R"
  },
  {
    "objectID": "22_python.html",
    "href": "22_python.html",
    "title": "파이썬",
    "section": "",
    "text": "1 파이썬"
  },
  {
    "objectID": "30_dashboard.html",
    "href": "30_dashboard.html",
    "title": "대쉬보드",
    "section": "",
    "text": "1 대쉬보드 개발 과정\n\n데이터 및 EDA\n모듈\n대쉬보드"
  },
  {
    "objectID": "31_models.html",
    "href": "31_models.html",
    "title": "기계학습",
    "section": "",
    "text": "1 기계학습"
  },
  {
    "objectID": "32_product.html",
    "href": "32_product.html",
    "title": "데이터과학 제품과 서비스",
    "section": "",
    "text": "1 데이터 제품개발"
  },
  {
    "objectID": "41_time_series.html",
    "href": "41_time_series.html",
    "title": "시계열 데이터",
    "section": "",
    "text": "1 시계열 데이터"
  },
  {
    "objectID": "42_network.html",
    "href": "42_network.html",
    "title": "네트워크 데이터",
    "section": "",
    "text": "1 네트워크 데이터"
  },
  {
    "objectID": "43_geospatial.html",
    "href": "43_geospatial.html",
    "title": "공간정보 데이터",
    "section": "",
    "text": "1 공간정보 데이터"
  },
  {
    "objectID": "44_text_mining.html",
    "href": "44_text_mining.html",
    "title": "텍스트 데이터",
    "section": "",
    "text": "1 텍스트 데이터"
  },
  {
    "objectID": "51_nlp.html",
    "href": "51_nlp.html",
    "title": "딥러닝 - NLP",
    "section": "",
    "text": "1 자연어 처리"
  },
  {
    "objectID": "52_vision.html",
    "href": "52_vision.html",
    "title": "딥러닝 - 비전",
    "section": "",
    "text": "1 딥러닝 기본기"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "데이터셋",
    "section": "",
    "text": "gapminder\n\nR 데이터 패키지: gapminder: Data from Gapminder\n\npalmer penguin\n\niris\nR built-in 내장 데이터: iris\n펭귄 vs 붓꽃 데이터\n\nBitData\n\n\n\n\n\nMNIST\n\n\n\n\n\nData Carpentry, 원데이터, weecology/portal-teachingdb\nDVD 대여 데이터베이스: GitHub, PostgreSQL Sample Database, PostgreSQL Sample Database, Load PostgreSQL Sample Database\nSoftware Carpentry: survey.db , DB 설치방법"
  },
  {
    "objectID": "data_chatGPT.html",
    "href": "data_chatGPT.html",
    "title": "chatGPT 데이터셋",
    "section": "",
    "text": "OpenAI에서 최근에 공개한 chatGPT와 함께 인공지능과 공존할 수 있는 방법을 모색해 봅니다.\n\n1 회귀모형\n\n\n\n\n\n\nwhat is the best dataset for regression?\n\n\n\n\n\nThere is no one “best” dataset for regression, as the appropriate dataset will depend on the specific problem you are trying to solve. Some common datasets that are often used for regression include the Boston Housing dataset, the California Housing dataset, and the Diabetes dataset. These datasets are well-studied and can be easily accessed by researchers, making them a good starting point for many regression tasks. Ultimately, the best dataset for your regression problem will depend on the specific problem you are trying to solve and the nature of the data you have available.\n\n\n\n\n\n\n\n\n\nhow about mpg dataset?\n\n\n\n\n\nThe mpg dataset is a dataset that contains information on the fuel efficiency of different cars. It is commonly used in regression tasks, as the goal is often to predict a car’s fuel efficiency based on various features such as its engine size and horsepower. This dataset can be a good choice for regression tasks, as it contains a large number of records and a variety of features that can be used to train a regression model. However, as with any dataset, it’s important to carefully evaluate the quality and relevance of the data before using it for your specific task.\n\n\n\n\n\n\n\n\n\ncan you show me a sample code with R?\n\n\n\n\n\nSure, here is a simple example of a linear regression model in R:\n\n# Load the necessary libraries\nlibrary(tidyverse)\n\n# Load the mpg dataset\ndata(\"mpg\")\n\n# Fit a linear regression model using the \"displ\" and \"hwy\" variables\nmodel <- lm(hwy ~ displ, data = mpg)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n# Predict the hwy fuel efficiency for a car with a displacement of 3\npredict(model, newdata = data.frame(displ = 3))\n\n       1 \n25.10588 \n\n\nThis code fits a linear regression model using the displ and hwy variables from the mpg dataset, and then uses the model to predict the fuel efficiency of a car with a displacement of 3. Note that this is a very simple example, and there are many different ways to perform regression in R. For more information and examples, you can refer to the R documentation and tutorials on regression.\n\n\n\n\n\n2 분류모형\n\n\n\n\n\n\nI am learning machine learning. In particular I’d like to build classification model. Could you recommend a suitable dataset for this purpose?\n\n\n\n\n\nSure, there are many datasets that you can use to build a classification model. One good dataset to start with is the Iris dataset, which is a popular dataset used in machine learning examples. It contains data on the different species of iris flowers, including their petal and sepal length and width. This dataset is small, so it’s easy to work with, but it’s also complex enough to provide a good challenge for a classification model. Other datasets that you might consider include the MNIST dataset for handwritten digit recognition, the CIFAR-10 dataset for object recognition, and the IMDB dataset for sentiment analysis. Ultimately, the best dataset for you to use will depend on your specific goals and needs.\n\n\n\n\n\n\n\n\n\nCan you show me sample python code for building classification machine learning model?\n\n\n\n\n\nHere is an example of how you might build a simple classification model using Python:\n\n# Import the necessary libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the iris dataset\niris = datasets.load_iris()\n\n# Split the data into features and labels\nX = iris.data\ny = iris.target\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nscore = model.score(X_test, y_test)\n\n# Print the accuracy of the model\nprint(\"Accuracy:\", score)\n\nThis code creates a logistic regression model and trains it on the Iris dataset. It then evaluates the model on the test data and prints the model’s accuracy. This is just one example of how you might build a classification model using Python – there are many other algorithms and approaches that you can use.\n\n\n\n\n\n\n\n\n\nCan you translate the python code above into R code with tidymodels?\n\n\n\n\n\nSure, here is an example of how you might translate the Python code above into R code using the tidymodels package:\n\n# Install and load the tidymodels package\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\n\n# Load the iris dataset\niris <- datasets::iris\n\n# Split the data into features and labels\nX <- iris %>% select(-Species)\ny <- iris %>% pull(Species)\n\n# Split the data into training and test sets\nsplits <- initial_split(iris, prop = 0.8)\n\n# Preprocess the data using recipe\nrec <- recipe(Species ~ ., data = iris) %>%\n  step_zv(all_predictors()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# Create a logistic regression model\nlr_mod <- logistic_reg() %>%\n  set_engine(\"glm\") \n\n# Train the model on the training data\nlr_fit <- lr_mod %>%\n  fit(rec, data = training(splits))\n\nError in new.env(parent = parent): use of NULL environment is defunct\n\n# Evaluate the model on the test data\nlr_pred <- lr_fit %>%\n  predict(new_data = testing(splits))\n\nError in predict(., new_data = testing(splits)): object 'lr_fit' not found\n\n# Print the accuracy of the model\nlr_pred %>% metrics(truth = testing(splits) %>% pull(Species), estimate = .pred_class) %>%\n  metric_set(acc)\n\nError in metrics(., truth = testing(splits) %>% pull(Species), estimate = .pred_class): object 'lr_pred' not found\n\n\nThis code uses the tidymodels package to create a logistic regression model and train it on the Iris dataset. It then evaluates the model on the test data and prints the model’s accuracy. This is just one example of how you might build a classification model using tidymodels – there are many other algorithms and approaches that you can use.\n\n\n\n\n\n3 차원축소\n\n\n\n\n\n\nI’d like to practice multivariate data analysis. In particular I am interested in principal component analysis. Can you recommend a dataset for this purpose?\n\n\n\n\n\nSure! There are many publicly available datasets that would be suitable for practicing principal component analysis. One example is the Iris dataset, which is a well-known dataset in the field of statistics and machine learning. It contains 150 samples of iris flowers, with each sample having four measurements: sepal length, sepal width, petal length, and petal width. These measurements are used to classify the iris flowers into three different species: Iris setosa, Iris virginica, and Iris versicolor.\n\n\n\n\n\n\n\n\n\nAlready I used the Iris data. Can you recommend another dataset, something interesting?\n\n\n\n\n\nSure! Here are a few other publicly available datasets that you could use for practicing principal component analysis:\n\nThe Wine dataset, which contains chemical analysis of 13 different wines grown in the same region in Italy. This dataset can be used to classify the wines based on their chemical properties.\nThe MNIST dataset, which contains images of handwritten digits. This dataset can be used to classify the digits based on their visual appearance.\nThe Human Activity Recognition dataset, which contains sensor readings from smartphones worn by 30 different subjects performing six different activities. This dataset can be used to classify the activities based on the sensor readings.\n\nThese are just a few examples - there are many other publicly available datasets that you could use for practicing principal component analysis. You can find many more by searching online for “multivariate datasets” or “principal component analysis datasets.”\n\n\n\n\n\n\n\n\n\nshow me a sample R code for the Wine dataset.\n\n\n\n\n\nHere is a sample R code that you could use to perform principal component analysis on the Wine dataset:\n\n# Load the wine dataset\ndata <- read.csv(\"wine.csv\")\n\n# Perform principal component analysis\npca <- prcomp(data, scale = TRUE)\n\n# Print the results\nprint(pca)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "데이터 과학 커리큘럼",
    "section": "",
    "text": "1 교육 전후\n체계적인 데이터 과학 교육전에는 실무 업무를 동시에 진행하면서 그때 그때 필요한 도구를 학습하여 적용하는 방식으로 진행된 반면 교육 후에는 다양한 도구를 그때 데이터의 상황과 문제에 맞춰 체계적으로 데이터 과학 제품을 개발하여 가치를 창출하고 유지발전시켜 나가게 됩니다.\n\n\n\n\n\n\n\n\n\n\n\n\n2 학습 내용\nJanssens (2021), Wickham & Grolemund (2016), Horst et al. (2020), Xie et al. (2020), Severance (2013), Wilson (2006), 한국R사용자회 (2022)\n\n\n\n\n\n\n  \n    \n      데이터 과학 교육과정\n    \n    \n  \n  \n    \n      일정\n      과목명\n      학습상세\n      데이터셋\n    \n  \n  \n    00주차\n환경설정\n학습 도구 설치 및 학습환경\n파머 펭귄, BitData\n    01주차\n디지털 글쓰기(보고서)\n마크다운 / Quarto\nBitData\n    02주차\n명령라인 데이터 분석\nShell\nSWC\n    03주차\n버전 제어\nGit / GitHub\n\n    04주차\n프로그래밍\nSQL\nData Carpentry, DVD 렌탈\n    05주차\n프로그래밍\nR\ngapminder\n    06주차\n프로그래밍\n파이썬\n\n    07주차\n대쉬보드\nFlexdashboard / Shinydashboard\n\n    08주차\n기계학습\ntidymodels / scikit-learn\n\n    09주차\n데이터 과학 제품\nRESTful API\n\n    10주차\n특수 데이터\n시계열(Time Series)\nKOSPI\n    11주차\n특수 데이터\n공간정보(Geospatial) / 텍스트 분석\n미디어오늘\n    12주차\n비정형 데이터\n텍스트와 이미지 (딥러닝)\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n참고문헌\n\nHorst, A. M., Hill, A. P., & Gorman, K. B. (2020). Palmerpenguins: Palmer archipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/\n\n\nJanssens, J. (2021). Data science at the command line. \" O’Reilly Media, Inc.\".\n\n\nSeverance, C. (2013). Python for informatics: Exploring information. CreateSpace.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. \" O’Reilly Media, Inc.\".\n\n\nWilson, G. (2006). Software carpentry: Getting scientists to write better code by making them more productive. Computing in Science & Engineering, 8(6), 66–69.\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman; Hall/CRC.\n\n\n한국R사용자회. (2022). R 텍스트마이닝. https://r2bit.com/book_tm/"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "프로젝트",
    "section": "",
    "text": "기획\n분석\n개발\n테스트\n운영\n…"
  }
]